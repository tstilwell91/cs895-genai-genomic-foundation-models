{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b491aab-bc4f-4a2d-85cf-ae893265d748",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment as needed in your runtime (internet required)\n",
    "%pip install -q transformers datasets peft accelerate scikit-learn matplotlib seaborn \\\n",
    "              pyfaidx google-cloud-storage cyvcf2\n",
    "\n",
    "# If using CUDA:\n",
    "# import torch\n",
    "# print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45fa0bc-ef6e-4df0-91fe-e0924051770c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, urllib.request, gzip, shutil, pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adfea8aa-a3d6-4a49-ae2b-76e6a3890861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: ./data/clinvar.vcf.gz\n",
      "Exists: ./data/clinvar.vcf.gz.tbi\n",
      "DATA_DIR: ./data\n",
      "VCF ready: True Index ready: True\n"
     ]
    }
   ],
   "source": [
    "# Download ClinVar\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Download ClinVar GRCh38 VCF + index\n",
    "vcf_url  = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz\"\n",
    "tbi_url  = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz.tbi\"\n",
    "vcf_path = os.path.join(DATA_DIR, \"clinvar.vcf.gz\")\n",
    "tbi_path = os.path.join(DATA_DIR, \"clinvar.vcf.gz.tbi\")\n",
    "\n",
    "def _dl(url, dest):\n",
    "    if not os.path.exists(dest):\n",
    "        print(f\"Downloading {url} -> {dest}\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "    else:\n",
    "        print(f\"Exists: {dest}\")\n",
    "\n",
    "_dl(vcf_url, vcf_path)\n",
    "_dl(tbi_url, tbi_path)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"VCF ready:\", os.path.exists(vcf_path), \"Index ready:\", os.path.exists(tbi_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22df4638-0d14-4bbd-bed7-9d02f9a6e8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept SNVs: 318801 | dropped (no binary CLNSIG): 2067808 | dropped (non-1bp alleles): 0 | dropped (low-conf rev): 1212581\n",
      "Saved: ./data/clinvar_snvs_clean.csv Rows: 318801\n",
      "Label counts:\n",
      " LABEL\n",
      "0    264534\n",
      "1     54267\n",
      "Name: count, dtype: int64\n",
      "  CHROM     POS REF ALT                CLNSIG  \\\n",
      "0     1  930204   G   A                Benign   \n",
      "1     1  930248   G   A         Likely_benign   \n",
      "2     1  930314   C   T                Benign   \n",
      "3     1  931042   C   T                Benign   \n",
      "4     1  935779   G   A  Benign/Likely_benign   \n",
      "\n",
      "                                              CLNREV       GENEINFO  LABEL  \n",
      "0  criteria_provided,_multiple_submitters,_no_con...  SAMD11:148398      0  \n",
      "1  criteria_provided,_multiple_submitters,_no_con...  SAMD11:148398      0  \n",
      "2  criteria_provided,_multiple_submitters,_no_con...  SAMD11:148398      0  \n",
      "3  criteria_provided,_multiple_submitters,_no_con...  SAMD11:148398      0  \n",
      "4  criteria_provided,_multiple_submitters,_no_con...  SAMD11:148398      0  \n"
     ]
    }
   ],
   "source": [
    "# Filter ClinVar dataset\n",
    "from cyvcf2 import VCF\n",
    "import re, csv\n",
    "\n",
    "# ----- Configs -----\n",
    "out_csv = os.path.join(DATA_DIR, \"clinvar_snvs_clean.csv\")\n",
    "\n",
    "# Map CLNSIG to binary label (1 = pathogenic, 0 = benign); drop everything else\n",
    "pat_re = re.compile(r\"(Pathogenic|Likely_pathogenic)\", re.IGNORECASE)\n",
    "ben_re = re.compile(r\"(Benign|Likely_benign)\", re.IGNORECASE)\n",
    "\n",
    "# Optional: keep only higher-confidence review statuses (set to False to keep all)\n",
    "HIGH_CONF_ONLY = True\n",
    "good_rev = {\n",
    "    \"criteria_provided,_multiple_submitters,_no_conflicts\",\n",
    "    \"reviewed_by_expert_panel\",\n",
    "    \"practice_guideline\",\n",
    "}\n",
    "\n",
    "# Parse\n",
    "vcf = VCF(vcf_path)\n",
    "rows = []\n",
    "kept, skipped_sig, skipped_len, skipped_conf = 0,0,0,0\n",
    "\n",
    "for rec in vcf:\n",
    "    # SNVs only\n",
    "    if not rec.is_snp:\n",
    "        continue\n",
    "\n",
    "    clnsig = (rec.INFO.get(\"CLNSIG\") or \"\").replace(\" \", \"_\")\n",
    "    rev    = (rec.INFO.get(\"CLNREVSTAT\") or \"\").replace(\" \", \"_\")\n",
    "    gene   = (rec.INFO.get(\"GENEINFO\") or \"\")\n",
    "\n",
    "    # Binary label mapping\n",
    "    is_pat = bool(pat_re.search(clnsig))\n",
    "    is_ben = bool(ben_re.search(clnsig))\n",
    "    if not (is_pat or is_ben):\n",
    "        skipped_sig += 1\n",
    "        continue\n",
    "    label = 1 if is_pat else 0\n",
    "\n",
    "    # Optional high-confidence filter\n",
    "    if HIGH_CONF_ONLY:\n",
    "        # allow any record whose CLNREVSTAT contains at least one \"good\" tag\n",
    "        rv = rev.lower()\n",
    "        if not any(tag in rv for tag in good_rev):\n",
    "            skipped_conf += 1\n",
    "            continue\n",
    "\n",
    "    # Split multi-allelics; keep only 1bp ref/alt (pure SNVs)\n",
    "    for alt in rec.ALT or []:\n",
    "        if len(rec.REF) != 1 or len(alt) != 1:\n",
    "            skipped_len += 1\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"CHROM\":   rec.CHROM,      # typically '1'..'22','X','Y','MT'\n",
    "            \"POS\":     rec.POS,        # 1-based\n",
    "            \"REF\":     rec.REF,\n",
    "            \"ALT\":     alt,\n",
    "            \"CLNSIG\":  clnsig,\n",
    "            \"CLNREV\":  rev,\n",
    "            \"GENEINFO\":gene,\n",
    "            \"LABEL\":   label           # 1=Pathogenic/Likely_pathogenic, 0=Benign/Likely_benign\n",
    "        })\n",
    "        kept += 1\n",
    "\n",
    "print(f\"Kept SNVs: {kept} | dropped (no binary CLNSIG): {skipped_sig} | \"\n",
    "      f\"dropped (non-1bp alleles): {skipped_len} | dropped (low-conf rev): {skipped_conf}\")\n",
    "\n",
    "# De-duplicate exact (CHROM,POS,REF,ALT) if desired\n",
    "df = pd.DataFrame(rows).drop_duplicates(subset=[\"CHROM\",\"POS\",\"REF\",\"ALT\"])\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv, \"Rows:\", len(df))\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"Label counts:\\n\", df[\"LABEL\"].value_counts())\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a23183-fd44-4138-813f-2ae80ac7e698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CHROM\n",
       "2     29804\n",
       "1     26248\n",
       "17    22870\n",
       "16    19121\n",
       "11    18952\n",
       "19    17635\n",
       "3     16875\n",
       "7     15999\n",
       "12    15466\n",
       "9     15063\n",
       "5     14928\n",
       "X     12735\n",
       "10    11908\n",
       "15    11349\n",
       "14     9977\n",
       "4      9664\n",
       "8      9219\n",
       "6      9060\n",
       "13     8200\n",
       "22     7133\n",
       "20     6687\n",
       "18     5486\n",
       "21     4087\n",
       "MT      331\n",
       "Y         4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print Chromosome counts\n",
    "df[\"CHROM\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "961a46c2-541b-4f3c-8c8a-6c8e66a7c46a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: ./ref/hg38.fa.gz\n",
      "Exists: ./ref/hg38.fa\n",
      "Reference ready: True | size (GB) ~ 3.27\n"
     ]
    }
   ],
   "source": [
    "# Download Reference\n",
    "import os, urllib.request, gzip, shutil\n",
    "\n",
    "REF_DIR = os.path.join(\".\", \"ref\")\n",
    "os.makedirs(REF_DIR, exist_ok=True)\n",
    "\n",
    "# UCSC hg38 (chr1..chr22, chrX, chrY, chrM) â€” good match for your CHROM values after mapping MT->chrM\n",
    "hg38_url = \"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\"\n",
    "fa_gz    = os.path.join(REF_DIR, \"hg38.fa.gz\")\n",
    "fa_path  = os.path.join(REF_DIR, \"hg38.fa\")\n",
    "\n",
    "def _dl(url, dest):\n",
    "    if not os.path.exists(dest):\n",
    "        print(f\"Downloading {url} -> {dest}\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "    else:\n",
    "        print(f\"Exists: {dest}\")\n",
    "\n",
    "# Download and gunzip (first time only)\n",
    "_dl(hg38_url, fa_gz)\n",
    "if not os.path.exists(fa_path):\n",
    "    print(f\"Unzipping {fa_gz} -> {fa_path} (this can take a few minutes)\")\n",
    "    with gzip.open(fa_gz, \"rb\") as f_in, open(fa_path, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "else:\n",
    "    print(f\"Exists: {fa_path}\")\n",
    "\n",
    "print(\"Reference ready:\", os.path.exists(fa_path), \"| size (GB) ~\", round(os.path.getsize(fa_path)/1e9, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a30ad5e-1d72-4b29-9f96-fad7824d91e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CHROM CHR_UCSC\n",
      "0     1     chr1\n",
      "1     1     chr1\n",
      "2     1     chr1\n",
      "3     1     chr1\n",
      "4     1     chr1\n",
      "CHROM       object\n",
      "POS          Int64\n",
      "REF         object\n",
      "ALT         object\n",
      "CLNSIG      object\n",
      "CLNREV      object\n",
      "GENEINFO    object\n",
      "LABEL         Int8\n",
      "CHR_UCSC    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load ClinVar SNVs and Normalize Chrom Names (robust dtype handling)\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "src_csv  = os.path.join(DATA_DIR, \"clinvar_snvs_clean.csv\")\n",
    "assert os.path.exists(src_csv), f\"Missing: {src_csv} (run the VCF parsing step first).\"\n",
    "\n",
    "# 1) Read as strings to avoid mixed-type inference warnings, then coerce numerics\n",
    "df = pd.read_csv(\n",
    "    src_csv,\n",
    "    dtype=str,            # read everything as string first\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Clean up any accidental index column\n",
    "for idx_col in [\"Unnamed: 0\", \"index\"]:\n",
    "    if idx_col in df.columns:\n",
    "        df = df.drop(columns=[idx_col])\n",
    "\n",
    "# Strip column-name whitespace just in case\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# 2) Coerce numeric columns\n",
    "for col, dtype in [(\"POS\", \"Int64\"), (\"LABEL\", \"Int8\")]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(dtype)\n",
    "\n",
    "# 3) Normalize chromosome names to UCSC FASTA names\n",
    "def normalize_chr(chrom: str) -> str:\n",
    "    \"\"\"\n",
    "    Map ClinVar CHROM values to UCSC FASTA names:\n",
    "      '1' -> 'chr1', ..., '22' -> 'chr22', 'X' -> 'chrX', 'Y' -> 'chrY', 'MT' -> 'chrM'\n",
    "    If already 'chr*', leave as is.\n",
    "    \"\"\"\n",
    "    if chrom is None or pd.isna(chrom):\n",
    "        return chrom\n",
    "    c = str(chrom).strip()\n",
    "    if not c.startswith(\"chr\"):\n",
    "        c = \"chr\" + c\n",
    "    if c == \"chrMT\":\n",
    "        c = \"chrM\"\n",
    "    return c\n",
    "\n",
    "df[\"CHR_UCSC\"] = df[\"CHROM\"].apply(normalize_chr)\n",
    "\n",
    "print(df[[\"CHROM\", \"CHR_UCSC\"]].head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48b8d577-d583-45c8-a837-3b89c006cc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built windows for 79825 variants; skipped 238976 due to chromosome/position mismatches.\n",
      "  CHROM     POS REF ALT  LABEL  \\\n",
      "0     1  930204   G   A      0   \n",
      "1     1  930248   G   A      0   \n",
      "2     1  931042   C   T      0   \n",
      "3     1  935779   G   A      0   \n",
      "4     1  935859   C   T      0   \n",
      "\n",
      "                                             ref_seq  \\\n",
      "0  CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...   \n",
      "1  CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...   \n",
      "2  CACTGACCCGAGACAGGTCTGCGCACGCCCTGCTATCCTGAGGCTG...   \n",
      "3  TCACACACAGAGCTAGGCACTCCCTGTGCCCAGGCTGGGCTCCAGC...   \n",
      "4  CTCGTTCTGCAGCCAGGACGGCAACCTTCCCACCCTCATATCCAGC...   \n",
      "\n",
      "                                             alt_seq  \n",
      "0  CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...  \n",
      "1  CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...  \n",
      "2  CACTGACCCGAGACAGGTCTGCGCACGCCCTGCTATCCTGAGGCTG...  \n",
      "3  TCACACACAGAGCTAGGCACTCCCTGTGCCCAGGCTGGGCTCCAGC...  \n",
      "4  CTCGTTCTGCAGCCAGGACGGCAACCTTCCCACCCTCATATCCAGC...  \n"
     ]
    }
   ],
   "source": [
    "# Extract REF/ALT windows (+- FLANK) with pyfaidx\n",
    "from pyfaidx import Fasta\n",
    "import numpy as np\n",
    "\n",
    "# configurable flank (Â±N bp)\n",
    "FLANK = 100  # try 50/100/200 as experiments\n",
    "MAX_ROWS = None  # e.g., set to 5000 for a quick test\n",
    "\n",
    "fa = Fasta(os.path.join(\"ref\",\"hg38.fa\"), as_raw=True, build_index=True)\n",
    "\n",
    "def fetch_windows(row, flank=FLANK):\n",
    "    \"\"\"\n",
    "    Returns (ref_window, alt_window) around POS for 1bp REF->ALT.\n",
    "    Performs a quick sanity check that reference base matches FASTA.\n",
    "    \"\"\"\n",
    "    chrom = row[\"CHR_UCSC\"]\n",
    "    pos   = int(row[\"POS\"])            # 1-based\n",
    "    ref   = str(row[\"REF\"])\n",
    "    alt   = str(row[\"ALT\"])\n",
    "\n",
    "    # window coordinates inclusive [start, end], 1-based for pyfaidx slicing\n",
    "    start = max(1, pos - flank)\n",
    "    end   = pos + flank\n",
    "\n",
    "    try:\n",
    "        window = fa[chrom][start:end]  # string slice from FASTA\n",
    "    except KeyError:\n",
    "        # chromosome absent in FASTA (shouldn't happen with UCSC hg38)\n",
    "        return None, None\n",
    "\n",
    "    window = str(window).upper()\n",
    "    # check the reference base at center (index relative to 'start')\n",
    "    center_idx = pos - start\n",
    "    if center_idx < 0 or center_idx >= len(window):\n",
    "        return None, None\n",
    "\n",
    "    # confirm the base matches expectation\n",
    "    if window[center_idx] != ref.upper():\n",
    "        # Sometimes reference mismatch can occur due to liftover/outdated positions.\n",
    "        # For now, skip mismatches to keep dataset clean.\n",
    "        return None, None\n",
    "\n",
    "    # build alt window by substituting at the center position\n",
    "    alt_window = window[:center_idx] + alt.upper() + window[center_idx+1:]\n",
    "\n",
    "    return window, alt_window\n",
    "\n",
    "# Apply (optionally on a subset for speed testing)\n",
    "work_df = df if MAX_ROWS is None else df.head(MAX_ROWS).copy()\n",
    "\n",
    "ref_windows, alt_windows = [], []\n",
    "skipped = 0\n",
    "\n",
    "for i, row in work_df.iterrows():\n",
    "    r, a = fetch_windows(row, FLANK)\n",
    "    if r is None or a is None:\n",
    "        ref_windows.append(np.nan)\n",
    "        alt_windows.append(np.nan)\n",
    "        skipped += 1\n",
    "    else:\n",
    "        ref_windows.append(r)\n",
    "        alt_windows.append(a)\n",
    "\n",
    "work_df[\"ref_seq\"] = ref_windows\n",
    "work_df[\"alt_seq\"] = alt_windows\n",
    "work_df = work_df.dropna(subset=[\"ref_seq\",\"alt_seq\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Built windows for {len(work_df)} variants; skipped {skipped} due to chromosome/position mismatches.\")\n",
    "print(work_df[[\"CHROM\",\"POS\",\"REF\",\"ALT\",\"LABEL\",\"ref_seq\",\"alt_seq\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19cefc09-8025-46d0-ac23-0a310d6d38fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./data/clinvar_seq_pairs_flank100.csv | rows: 79825\n"
     ]
    }
   ],
   "source": [
    "# Save ready-to-tokenize dataset\n",
    "out_csv = os.path.join(DATA_DIR, f\"clinvar_seq_pairs_flank{FLANK}.csv\")\n",
    "work_df.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv, \"| rows:\", len(work_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7cc993-79cb-45a8-a22c-d9c6ddd0bc40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Out of ~318 k ClinVar SNVs, ~80 k (25%) aligned perfectly to the reference genome (hg38.fa) after validation. The remaining variants likely represent assembly or coordinate mismatches.\n",
    "# I'm going to try NCBI's GRCh38 reference instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2c1883-3c9f-44b7-a205-64c409758c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: ./ref/GRCh38.p14_genomic.fna.gz\n",
      "Exists: ./ref/GRCh38.p14_genomic.fna\n",
      "NCBI FASTA ready: True | size (GB) ~ 3.34\n"
     ]
    }
   ],
   "source": [
    "# NCBI GRCh38 (RefSeq) â€” primary assembly FASTA\n",
    "# This file includes chromosomes and many scaffolds; we will auto-detect the\n",
    "# primary chromosomes by parsing headers ('chromosome 1', ..., 'X', 'Y', 'mitochondrion').\n",
    "\n",
    "import os, urllib.request, gzip, shutil, pathlib\n",
    "\n",
    "REF_DIR = os.path.join(\".\", \"ref\")\n",
    "os.makedirs(REF_DIR, exist_ok=True)\n",
    "\n",
    "# RefSeq assembly (p14 as of writing) â€” update URL if needed later\n",
    "NCBI_URL = (\"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/\"\n",
    "            \"GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz\")\n",
    "ncbi_gz   = os.path.join(REF_DIR, \"GRCh38.p14_genomic.fna.gz\")\n",
    "ncbi_fa   = os.path.join(REF_DIR, \"GRCh38.p14_genomic.fna\")\n",
    "\n",
    "def _dl(url, dest):\n",
    "    if not os.path.exists(dest):\n",
    "        print(f\"Downloading {url} -> {dest}\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "    else:\n",
    "        print(f\"Exists: {dest}\")\n",
    "\n",
    "_dl(NCBI_URL, ncbi_gz)\n",
    "\n",
    "if not os.path.exists(ncbi_fa):\n",
    "    print(f\"Unzipping {ncbi_gz} -> {ncbi_fa} (this can take a few minutes)\")\n",
    "    with gzip.open(ncbi_gz, \"rb\") as f_in, open(ncbi_fa, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "else:\n",
    "    print(f\"Exists: {ncbi_fa}\")\n",
    "\n",
    "print(\"NCBI FASTA ready:\", os.path.exists(ncbi_fa), \"| size (GB) ~\", round(os.path.getsize(ncbi_fa)/1e9, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9b1de9-a968-472a-8071-2af66b3e3a70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered chromosomeâ†’accession mapping:\n",
      "   X -> NC_000023.11\n",
      "   Y -> NC_000024.10\n",
      "  MT -> NC_012920.1\n",
      "   1 -> NC_000001.11\n",
      "  10 -> NC_000010.11\n",
      "  11 -> NC_000011.10\n",
      "  12 -> NC_000012.12\n",
      "  13 -> NC_000013.11\n",
      "  14 -> NC_000014.9\n",
      "  15 -> NC_000015.10\n",
      "  16 -> NC_000016.10\n",
      "  17 -> NC_000017.11\n",
      "  18 -> NC_000018.10\n",
      "  19 -> NC_000019.10\n",
      "   2 -> NC_000002.12\n",
      "  20 -> NC_000020.11\n",
      "  21 -> NC_000021.9\n",
      "  22 -> NC_000022.11\n",
      "   3 -> NC_000003.12\n",
      "   4 -> NC_000004.12\n",
      "   5 -> NC_000005.10\n",
      "   6 -> NC_000006.12\n",
      "   7 -> NC_000007.14\n",
      "   8 -> NC_000008.11\n",
      "   9 -> NC_000009.12\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "fa_path = Path(ncbi_fa)\n",
    "assert fa_path.exists(), \"NCBI FASTA not found â€” run the previous cell first.\"\n",
    "\n",
    "# We'll scan headers to discover accessions and their chromosome names\n",
    "chrom_to_acc = {}   # e.g., {'1': 'NC_000001.11', '2': 'NC_000002.12', ..., 'MT': 'NC_012920.1'}\n",
    "\n",
    "want = {str(i) for i in range(1,23)} | {\"X\",\"Y\",\"MT\"}  # target set\n",
    "\n",
    "header_re = re.compile(r\"^>(\\S+).+?(chromosome\\s+([0-9XY]{1,2})|mitochondrion)\", re.IGNORECASE)\n",
    "\n",
    "with open(fa_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if not line.startswith(\">\"):\n",
    "            continue\n",
    "        m = header_re.search(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        acc = m.group(1)\n",
    "        chrom_label = m.group(3)\n",
    "        if chrom_label:\n",
    "            key = chrom_label.upper()\n",
    "        else:\n",
    "            # If 'mitochondrion' matched\n",
    "            key = \"MT\"\n",
    "        if key in want and key not in chrom_to_acc:\n",
    "            chrom_to_acc[key] = acc\n",
    "\n",
    "print(\"Discovered chromosomeâ†’accession mapping:\")\n",
    "for k in sorted(chrom_to_acc, key=lambda x: (x not in {\"X\",\"Y\",\"MT\"}, x if x.isdigit() else 100)):\n",
    "    print(f\"  {k:>2} -> {chrom_to_acc[k]}\")\n",
    "\n",
    "missing = want - set(chrom_to_acc.keys())\n",
    "if missing:\n",
    "    print(\"WARNING: Did not find entries for:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda4c594-50fa-435a-99a7-d9ec8308f7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CHROM      NCBI_SEQ\n",
      "0     1  NC_000001.11\n",
      "1     1  NC_000001.11\n",
      "2     1  NC_000001.11\n",
      "3     1  NC_000001.11\n",
      "4     1  NC_000001.11\n",
      "Rows with missing NCBI mapping: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os\n",
    "\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "src_csv  = os.path.join(DATA_DIR, \"clinvar_snvs_clean.csv\")\n",
    "assert os.path.exists(src_csv), f\"Missing: {src_csv} (run the VCF parsing step first).\"\n",
    "\n",
    "df = pd.read_csv(src_csv, dtype=str)\n",
    "# Coerce numerics\n",
    "df[\"POS\"] = pd.to_numeric(df[\"POS\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"LABEL\"] = pd.to_numeric(df[\"LABEL\"], errors=\"coerce\").astype(\"Int8\")\n",
    "\n",
    "def to_ncbi_name(chrom: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Convert ClinVar CHROM ('1'..'22','X','Y','MT') to NCBI accession using parsed map.\n",
    "    Returns accession string like 'NC_000001.11' or None if unavailable.\n",
    "    \"\"\"\n",
    "    if chrom is None:\n",
    "        return None\n",
    "    key = str(chrom).strip().upper()\n",
    "    return chrom_to_acc.get(key)\n",
    "\n",
    "df[\"NCBI_SEQ\"] = df[\"CHROM\"].apply(to_ncbi_name)\n",
    "print(df[[\"CHROM\",\"NCBI_SEQ\"]].head())\n",
    "print(\"Rows with missing NCBI mapping:\", df[\"NCBI_SEQ\"].isna().sum())\n",
    "df = df.dropna(subset=[\"NCBI_SEQ\",\"POS\",\"REF\",\"ALT\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd4dfcd-3367-415b-be28-25f1e2c68087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built windows for 79825 variants; skipped 238976 due to reference mismatches or contigs.\n",
      "  CHROM      NCBI_SEQ     POS REF ALT  LABEL  \\\n",
      "0     1  NC_000001.11  930204   G   A      0   \n",
      "1     1  NC_000001.11  930248   G   A      0   \n",
      "2     1  NC_000001.11  931042   C   T      0   \n",
      "3     1  NC_000001.11  935779   G   A      0   \n",
      "4     1  NC_000001.11  935859   C   T      0   \n",
      "\n",
      "                                             ref_seq  \\\n",
      "0  CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...   \n",
      "1  CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...   \n",
      "2  CACTGACCCGAGACAGGTCTGCGCACGCCCTGCTATCCTGAGGCTG...   \n",
      "3  TCACACACAGAGCTAGGCACTCCCTGTGCCCAGGCTGGGCTCCAGC...   \n",
      "4  CTCGTTCTGCAGCCAGGACGGCAACCTTCCCACCCTCATATCCAGC...   \n",
      "\n",
      "                                             alt_seq  \n",
      "0  CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...  \n",
      "1  CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...  \n",
      "2  CACTGACCCGAGACAGGTCTGCGCACGCCCTGCTATCCTGAGGCTG...  \n",
      "3  TCACACACAGAGCTAGGCACTCCCTGTGCCCAGGCTGGGCTCCAGC...  \n",
      "4  CTCGTTCTGCAGCCAGGACGGCAACCTTCCCACCCTCATATCCAGC...  \n"
     ]
    }
   ],
   "source": [
    "from pyfaidx import Fasta\n",
    "import numpy as np\n",
    "\n",
    "FLANK = 100     # adjust (e.g., 50/100/200) as you wish\n",
    "MAX_ROWS = None # set to small int for quick test, None for all\n",
    "\n",
    "fa_ncbi = Fasta(str(fa_path), as_raw=True, build_index=True)\n",
    "\n",
    "def fetch_windows_ncbi(row, flank=FLANK):\n",
    "    \"\"\"\n",
    "    Extract ref/alt windows from NCBI GRCh38 using accession names.\n",
    "    Validates that REF matches the FASTA at the center position.\n",
    "    \"\"\"\n",
    "    acc = row[\"NCBI_SEQ\"]\n",
    "    pos = int(row[\"POS\"])   # 1-based\n",
    "    ref = str(row[\"REF\"]).upper()\n",
    "    alt = str(row[\"ALT\"]).upper()\n",
    "\n",
    "    start = max(1, pos - flank)\n",
    "    end   = pos + flank\n",
    "\n",
    "    try:\n",
    "        window = fa_ncbi[acc][start:end]\n",
    "    except KeyError:\n",
    "        return None, None\n",
    "\n",
    "    window = str(window).upper()\n",
    "    center_idx = pos - start\n",
    "    if center_idx < 0 or center_idx >= len(window):\n",
    "        return None, None\n",
    "\n",
    "    if window[center_idx] != ref:\n",
    "        return None, None\n",
    "\n",
    "    alt_window = window[:center_idx] + alt + window[center_idx+1:]\n",
    "    return window, alt_window\n",
    "\n",
    "work_df = df if MAX_ROWS is None else df.head(MAX_ROWS).copy()\n",
    "\n",
    "ref_windows, alt_windows = [], []\n",
    "skipped = 0\n",
    "for _, row in work_df.iterrows():\n",
    "    r, a = fetch_windows_ncbi(row, FLANK)\n",
    "    if r is None or a is None:\n",
    "        ref_windows.append(np.nan)\n",
    "        alt_windows.append(np.nan)\n",
    "        skipped += 1\n",
    "    else:\n",
    "        ref_windows.append(r)\n",
    "        alt_windows.append(a)\n",
    "\n",
    "work_df[\"ref_seq\"] = ref_windows\n",
    "work_df[\"alt_seq\"] = alt_windows\n",
    "work_df = work_df.dropna(subset=[\"ref_seq\", \"alt_seq\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Built windows for {len(work_df)} variants; skipped {skipped} due to reference mismatches or contigs.\")\n",
    "print(work_df[[\"CHROM\",\"NCBI_SEQ\",\"POS\",\"REF\",\"ALT\",\"LABEL\",\"ref_seq\",\"alt_seq\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "440061f9-6081-4e04-8c8d-b27caa609b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./data/clinvar_seq_pairs_ncbi_flank100.csv | rows: 79825\n"
     ]
    }
   ],
   "source": [
    "out_csv_ncbi = os.path.join(DATA_DIR, f\"clinvar_seq_pairs_ncbi_flank{FLANK}.csv\")\n",
    "work_df.to_csv(out_csv_ncbi, index=False)\n",
    "print(\"Saved:\", out_csv_ncbi, \"| rows:\", len(work_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85319786-2568-4ab3-b69b-c9d069a1c02d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 79825\n",
      "CHROM               object\n",
      "POS                  Int64\n",
      "REF                 object\n",
      "ALT                 object\n",
      "CLNSIG              object\n",
      "CLNREV              object\n",
      "GENEINFO            object\n",
      "LABEL                 Int8\n",
      "NCBI_SEQ            object\n",
      "ref_seq     string[python]\n",
      "alt_seq     string[python]\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>CLNSIG</th>\n",
       "      <th>CLNREV</th>\n",
       "      <th>GENEINFO</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>NCBI_SEQ</th>\n",
       "      <th>ref_seq</th>\n",
       "      <th>alt_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>930204</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>Benign</td>\n",
       "      <td>criteria_provided,_multiple_submitters,_no_con...</td>\n",
       "      <td>SAMD11:148398</td>\n",
       "      <td>0</td>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...</td>\n",
       "      <td>CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>930248</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>Likely_benign</td>\n",
       "      <td>criteria_provided,_multiple_submitters,_no_con...</td>\n",
       "      <td>SAMD11:148398</td>\n",
       "      <td>0</td>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...</td>\n",
       "      <td>CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM     POS REF ALT         CLNSIG  \\\n",
       "0     1  930204   G   A         Benign   \n",
       "1     1  930248   G   A  Likely_benign   \n",
       "\n",
       "                                              CLNREV       GENEINFO  LABEL  \\\n",
       "0  criteria_provided,_multiple_submitters,_no_con...  SAMD11:148398      0   \n",
       "1  criteria_provided,_multiple_submitters,_no_con...  SAMD11:148398      0   \n",
       "\n",
       "       NCBI_SEQ                                            ref_seq  \\\n",
       "0  NC_000001.11  CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...   \n",
       "1  NC_000001.11  CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...   \n",
       "\n",
       "                                             alt_seq  \n",
       "0  CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...  \n",
       "1  CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for DNABERT fine-tuning\n",
    "# Config and Load sequence pairs\n",
    "import os, pandas as pd\n",
    "\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "FLANK = 100\n",
    "K = 6\n",
    "\n",
    "src_csv = os.path.join(DATA_DIR, f\"clinvar_seq_pairs_ncbi_flank{FLANK}.csv\")\n",
    "assert os.path.exists(src_csv), f\"Missing: {src_csv}\"\n",
    "\n",
    "# Read everything as string first; prevents DtypeWarning\n",
    "df = pd.read_csv(src_csv, dtype=str, low_memory=False)\n",
    "\n",
    "# Clean stray index cols if present\n",
    "for c in (\"Unnamed: 0\", \"index\"):\n",
    "    if c in df.columns:\n",
    "        df = df.drop(columns=[c])\n",
    "\n",
    "# Coerce numeric columns (nullable dtypes keep NaN if any)\n",
    "if \"POS\" in df.columns:\n",
    "    df[\"POS\"] = pd.to_numeric(df[\"POS\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"LABEL\" in df.columns:\n",
    "    df[\"LABEL\"] = pd.to_numeric(df[\"LABEL\"], errors=\"coerce\").astype(\"Int8\")\n",
    "\n",
    "# Make sure sequence/text columns are strings\n",
    "for c in (\"ref_seq\", \"alt_seq\"):\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"string\")\n",
    "\n",
    "df = df.dropna(subset=[\"alt_seq\", \"ref_seq\", \"LABEL\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(df.dtypes)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2923bec6-c522-44c9-99e9-d4d1d24bcdc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGCCTGCCGCCCGGAACCTGAAGAAGGAGCGAACTCCCAGCTTCTCTGCCAGCGATGGTGACAGCGACGGGAGTGGCCCCACCTGTGGGCGGCGGCCAGGCTTGAAGCAGGAG\n",
      "CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGCCTGCCGCCCGAAACCTGAAGAAGGAGCGAACTCCCAGCTTCTCTGCCAGCGATGGTGACAGCGACGGGAGTGGCCCCACCTGTGGGCGGCGGCCAGGCTTGAAGCAGGAG\n"
     ]
    }
   ],
   "source": [
    "# Verifying no whitespace in sequences. Just Jupyter formatting\n",
    "print(df[\"ref_seq\"].iloc[0])\n",
    "print(df[\"alt_seq\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326818ba-bc8c-43da-b7d9-7dea0aa4312f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k-mer Utilities\n",
    "import re\n",
    "\n",
    "VALID = set(\"ACGT\")\n",
    "\n",
    "def clean_seq(seq: str) -> str:\n",
    "    \"\"\"Uppercase and replace any non-ACGT with 'A' (simple, fast).\"\"\"\n",
    "    s = str(seq).upper()\n",
    "    return \"\".join(ch if ch in VALID else \"A\" for ch in s)\n",
    "\n",
    "def kmerize(seq: str, k: int = 6) -> str:\n",
    "    \"\"\"Return space-separated overlapping k-mers (DNABERT input).\"\"\"\n",
    "    s = clean_seq(seq)\n",
    "    if len(s) < k:\n",
    "        return \"\"  # will be dropped later\n",
    "    return \" \".join(s[i:i+k] for i in range(len(s) - k + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "091965a6-941f-4ec0-929a-a1e1330b9214",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 rows with too-short sequences for k=6.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>text_alt_k6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>930204</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>CCCACC CCACCT CACCTT ACCTTC CCTTCC CTTCCT TTCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>930248</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>CACCAG ACCAGA CCAGAA CAGAAC AGAACC GAACCG AACC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM     POS REF ALT  LABEL  \\\n",
       "0     1  930204   G   A      0   \n",
       "1     1  930248   G   A      0   \n",
       "\n",
       "                                         text_alt_k6  \n",
       "0  CCCACC CCACCT CACCTT ACCTTC CCTTCC CTTCCT TTCC...  \n",
       "1  CACCAG ACCAGA CCAGAA CAGAAC AGAACC GAACCG AACC...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Built DNABERT text columns (ALT Baseline + optional REF)\n",
    "# ALT-only baseline (recommended first experiment)\n",
    "df[\"text_alt_k6\"] = df[\"alt_seq\"].map(lambda s: kmerize(s, k=K))\n",
    "\n",
    "# (Optional) REF text as well, if you plan paired experiments later\n",
    "df[\"text_ref_k6\"] = df[\"ref_seq\"].map(lambda s: kmerize(s, k=K))\n",
    "\n",
    "# Drop any rows where k-merization failed (too short, etc.)\n",
    "before = len(df)\n",
    "df = df[(df[\"text_alt_k6\"].str.len() > 0)]\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} rows with too-short sequences for k={K}.\")\n",
    "df[[\"CHROM\",\"POS\",\"REF\",\"ALT\",\"LABEL\",\"text_alt_k6\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd8b8880-4ed1-4238-bc41-880a2198dadb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "   ./data/prepared_k6_alt_only_all.csv (rows=79825)\n",
      "   ./data/prepared_k6_alt_only_train.csv (rows=63860)\n",
      "   ./data/prepared_k6_alt_only_val.csv (rows=15965)\n",
      "Label balance (all):\n",
      " LABEL\n",
      "0    0.836\n",
      "1    0.164\n",
      "Name: proportion, dtype: Float64\n"
     ]
    }
   ],
   "source": [
    "# Stratified train/val split and save CSVs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "keep_cols = [\"CHROM\",\"POS\",\"REF\",\"ALT\",\"LABEL\",\"text_alt_k6\",\"text_ref_k6\",\"ref_seq\",\"alt_seq\"]\n",
    "prepared = df[keep_cols].copy()\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    prepared,\n",
    "    test_size=0.2,\n",
    "    stratify=prepared[\"LABEL\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "prep_all  = os.path.join(DATA_DIR, f\"prepared_k{K}_alt_only_all.csv\")\n",
    "prep_tr   = os.path.join(DATA_DIR, f\"prepared_k{K}_alt_only_train.csv\")\n",
    "prep_val  = os.path.join(DATA_DIR, f\"prepared_k{K}_alt_only_val.csv\")\n",
    "\n",
    "prepared.to_csv(prep_all, index=False)\n",
    "train_df.to_csv(prep_tr, index=False)\n",
    "val_df.to_csv(prep_val, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"  \", prep_all, f\"(rows={len(prepared)})\")\n",
    "print(\"  \", prep_tr,  f\"(rows={len(train_df)})\")\n",
    "print(\"  \", prep_val, f\"(rows={len(val_df)})\")\n",
    "print(\"Label balance (all):\\n\", prepared[\"LABEL\"].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26b7d448-73e0-4dee-a3f4-d53596b12411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6624c10baa46e5b42fd011a930a84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hugging Face Login\n",
    "# Only needed if your environment blocks anonymous HF downloads\n",
    "# Get a token from https://huggingface.co/settings/tokens\n",
    "# and paste it when prompted.\n",
    "from huggingface_hub import login\n",
    "try:\n",
    "    login()  # or login(token=\"hf_...\") to pass programmatically\n",
    "except Exception as e:\n",
    "    print(\"HF login skipped or not required:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff12c723-b389-4f5e-8bf0-2ff68bbc4398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15593bf8818944cebe4ad8c83e7ba780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from: zhihan1996/DNA_bert_6\n",
      "Tokenized input_ids shape: torch.Size([1, 512])\n",
      "First 10 tokens: [2, 2703, 2606, 2218, 667, 2655, 2414, 1451, 1694, 2667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load DNABERT tokenizer \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_CANDIDATES = [\n",
    "    \"zhihan1996/DNA_bert_6\",     # official DNABERT (k=6)\n",
    "    \"armheb/DNA_bert_6\",         # public mirror\n",
    "    # add more fallbacks here if needed\n",
    "]\n",
    "\n",
    "tok = None\n",
    "last_err = None\n",
    "for mid in MODEL_CANDIDATES:\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(mid, use_fast=False)  # DNABERT uses a classic BertTokenizer\n",
    "        MODEL_NAME = mid\n",
    "        print(\"Loaded tokenizer from:\", mid)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "        print(f\"Failed to load {mid}: {e}\")\n",
    "\n",
    "if tok is None:\n",
    "    raise last_err\n",
    "\n",
    "# Your existing sanity check\n",
    "sample = prepared[\"text_alt_k6\"].iloc[0]\n",
    "enc = tok(sample, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "print(\"Tokenized input_ids shape:\", enc[\"input_ids\"].shape)\n",
    "print(\"First 10 tokens:\", enc[\"input_ids\"][0, :10].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7f0b3e4-b5ca-4ce1-aff4-103ba9ae0555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now let's train. \n",
    "# 1. Baseline full fine-tune\n",
    "# 2. Weighted Loss (to handle the 83.6/16.4 class imbalance. May address this later\n",
    "# 3. LoRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b249e1d4-04f3-4897-927c-a099346fe87f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.44.2 in /opt/conda/lib/python3.10/site-packages (4.57.0)\n",
      "Requirement already satisfied: accelerate>=0.34.2 in /opt/conda/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: peft>=0.13.2 in /opt/conda/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: datasets>=2.20 in /opt/conda/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.44.2) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.44.2) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.44.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.44.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.44.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.44.2) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.44.2) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.44.2) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.44.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.44.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.2) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.2) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.2) (1.1.10)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.2) (5.9.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.2) (2.8.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20) (3.13.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.20) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.20) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.20) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.20) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.20) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.20) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.44.2) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.44.2) (2.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.2) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.10/site-packages (from triton==3.4.0->torch>=2.0.0->accelerate>=0.34.2) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.34.2) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=2.20) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=2.20) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.2) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Transformers: 4.57.0\n",
      "Torch: 2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "# One-time fixing packages\n",
    "#%pip install -U \"transformers>=4.44.2\" \"accelerate>=0.34.2\" \"peft>=0.13.2\" \"datasets>=2.20\"\n",
    "\n",
    "#import transformers, torch\n",
    "#print(\"Transformers:\", transformers.__version__)\n",
    "#print(\"Torch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ea195-4524-4d8f-89ff-d92733df7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time fix for transformers version\n",
    "\n",
    "#%pip uninstall -y transformers\n",
    "#%pip install \"transformers==4.44.2\"  # known-good, definitely has evaluation_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81918894-6ea8-4b64-b31f-d60403fb1d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import inspect\n",
    "\"evaluation_strategy\" in inspect.signature(TrainingArguments.__init__).parameters\n",
    "# should print: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3531276-67c2-4087-af11-821624158d58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNA_bert_6 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. Baseline full fine-tune\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "MODEL_NAME = mid  # from the tokenizer cell you just ran\n",
    "tokenizer = tok\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "class KmerTextDataset(Dataset):\n",
    "    def __init__(self, df, text_col=\"text_alt_k6\", label_col=\"LABEL\", max_length=512):\n",
    "        self.texts = df[text_col].tolist()\n",
    "        self.labels = df[label_col].astype(int).tolist()\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = KmerTextDataset(train_df)\n",
    "val_ds   = KmerTextDataset(val_df)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def compute_classification_metrics(y_true, y_prob):\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    return {\"auc\": auc, \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "def hf_compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1)[:,1].numpy()\n",
    "    return compute_classification_metrics(labels, probs)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/dnabert_fullft\",\n",
    "    num_train_epochs=1,                 # start with 1 for smoke test; then 3â€“5\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auc\",        # since your compute_metrics returns 'auc'\n",
    "    greater_is_better=True,\n",
    "    report_to=[],                       # avoids wandb/autologging surprises\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=hf_compute_metrics,\n",
    ")\n",
    "\n",
    "# trainer.train()   # <-- run when ready\n",
    "# preds = trainer.predict(val_ds)\n",
    "# y_prob = torch.softmax(torch.tensor(preds.predictions), dim=1)[:,1].numpy()\n",
    "# metrics = compute_classification_metrics(preds.label_ids, y_prob); metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f4bec-fc0a-4fa8-abe8-c6e5fd27f51a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
