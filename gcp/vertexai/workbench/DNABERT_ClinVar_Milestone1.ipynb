{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b491aab-bc4f-4a2d-85cf-ae893265d748",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Cleanup these first three blocks. Consolidate\n",
    "# Uncomment as needed in your runtime (internet required)\n",
    "%pip install -q transformers datasets peft accelerate scikit-learn matplotlib seaborn \\\n",
    "              pyfaidx google-cloud-storage cyvcf2\n",
    "\n",
    "# If using CUDA:\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "     print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9e05b-260f-46f2-9a98-2f42a4ba4c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One-time fixing packages\n",
    "%pip install -U \"transformers>=4.44.2\" \"accelerate>=0.34.2\" \"peft>=0.13.2\" \"datasets>=2.20\"\n",
    "\n",
    "import transformers, torch\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Torch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420290d-553b-44f4-b127-199447abaee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One-time fix for transformers version\n",
    "\n",
    "%pip uninstall -y transformers\n",
    "%pip install \"transformers==4.44.2\"  # known-good, definitely has evaluation_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45fa0bc-ef6e-4df0-91fe-e0924051770c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, urllib.request, gzip, shutil, pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfea8aa-a3d6-4a49-ae2b-76e6a3890861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: ./data/clinvar.vcf.gz\n",
      "Exists: ./data/clinvar.vcf.gz.tbi\n",
      "DATA_DIR: ./data\n",
      "VCF ready: True Index ready: True\n"
     ]
    }
   ],
   "source": [
    "# Download ClinVar\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Download ClinVar GRCh38 VCF + index\n",
    "vcf_url  = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz\"\n",
    "tbi_url  = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz.tbi\"\n",
    "vcf_path = os.path.join(DATA_DIR, \"clinvar.vcf.gz\")\n",
    "tbi_path = os.path.join(DATA_DIR, \"clinvar.vcf.gz.tbi\")\n",
    "\n",
    "def _dl(url, dest):\n",
    "    if not os.path.exists(dest):\n",
    "        print(f\"Downloading {url} -> {dest}\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "    else:\n",
    "        print(f\"Exists: {dest}\")\n",
    "\n",
    "_dl(vcf_url, vcf_path)\n",
    "_dl(tbi_url, tbi_path)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"VCF ready:\", os.path.exists(vcf_path), \"Index ready:\", os.path.exists(tbi_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df4638-0d14-4bbd-bed7-9d02f9a6e8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter ClinVar dataset\n",
    "from cyvcf2 import VCF\n",
    "import re, csv\n",
    "\n",
    "# ----- Configs -----\n",
    "out_csv = os.path.join(DATA_DIR, \"clinvar_snvs_clean.csv\")\n",
    "\n",
    "# Map CLNSIG to binary label (1 = pathogenic, 0 = benign); drop everything else\n",
    "pat_re = re.compile(r\"(Pathogenic|Likely_pathogenic)\", re.IGNORECASE)\n",
    "ben_re = re.compile(r\"(Benign|Likely_benign)\", re.IGNORECASE)\n",
    "\n",
    "# Optional: keep only higher-confidence review statuses (set to False to keep all)\n",
    "HIGH_CONF_ONLY = True\n",
    "good_rev = {\n",
    "    \"criteria_provided,_multiple_submitters,_no_conflicts\",\n",
    "    \"reviewed_by_expert_panel\",\n",
    "    \"practice_guideline\",\n",
    "}\n",
    "\n",
    "# Parse\n",
    "vcf = VCF(vcf_path)\n",
    "rows = []\n",
    "kept, skipped_sig, skipped_len, skipped_conf = 0,0,0,0\n",
    "\n",
    "for rec in vcf:\n",
    "    # SNVs only\n",
    "    if not rec.is_snp:\n",
    "        continue\n",
    "\n",
    "    clnsig = (rec.INFO.get(\"CLNSIG\") or \"\").replace(\" \", \"_\")\n",
    "    rev    = (rec.INFO.get(\"CLNREVSTAT\") or \"\").replace(\" \", \"_\")\n",
    "    gene   = (rec.INFO.get(\"GENEINFO\") or \"\")\n",
    "\n",
    "    # Binary label mapping\n",
    "    is_pat = bool(pat_re.search(clnsig))\n",
    "    is_ben = bool(ben_re.search(clnsig))\n",
    "    if not (is_pat or is_ben):\n",
    "        skipped_sig += 1\n",
    "        continue\n",
    "    label = 1 if is_pat else 0\n",
    "\n",
    "    # Optional high-confidence filter\n",
    "    if HIGH_CONF_ONLY:\n",
    "        # allow any record whose CLNREVSTAT contains at least one \"good\" tag\n",
    "        rv = rev.lower()\n",
    "        if not any(tag in rv for tag in good_rev):\n",
    "            skipped_conf += 1\n",
    "            continue\n",
    "\n",
    "    # Split multi-allelics; keep only 1bp ref/alt (pure SNVs)\n",
    "    for alt in rec.ALT or []:\n",
    "        if len(rec.REF) != 1 or len(alt) != 1:\n",
    "            skipped_len += 1\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"CHROM\":   rec.CHROM,      # typically '1'..'22','X','Y','MT'\n",
    "            \"POS\":     rec.POS,        # 1-based\n",
    "            \"REF\":     rec.REF,\n",
    "            \"ALT\":     alt,\n",
    "            \"CLNSIG\":  clnsig,\n",
    "            \"CLNREV\":  rev,\n",
    "            \"GENEINFO\":gene,\n",
    "            \"LABEL\":   label           # 1=Pathogenic/Likely_pathogenic, 0=Benign/Likely_benign\n",
    "        })\n",
    "        kept += 1\n",
    "\n",
    "print(f\"Kept SNVs: {kept} | dropped (no binary CLNSIG): {skipped_sig} | \"\n",
    "      f\"dropped (non-1bp alleles): {skipped_len} | dropped (low-conf rev): {skipped_conf}\")\n",
    "\n",
    "# De-duplicate exact (CHROM,POS,REF,ALT) if desired\n",
    "df = pd.DataFrame(rows).drop_duplicates(subset=[\"CHROM\",\"POS\",\"REF\",\"ALT\"])\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv, \"Rows:\", len(df))\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"Label counts:\\n\", df[\"LABEL\"].value_counts())\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a23183-fd44-4138-813f-2ae80ac7e698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print Chromosome counts\n",
    "df[\"CHROM\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a46c2-541b-4f3c-8c8a-6c8e66a7c46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download Reference\n",
    "import os, urllib.request, gzip, shutil\n",
    "\n",
    "REF_DIR = os.path.join(\".\", \"ref\")\n",
    "os.makedirs(REF_DIR, exist_ok=True)\n",
    "\n",
    "# UCSC hg38 (chr1..chr22, chrX, chrY, chrM) — good match for your CHROM values after mapping MT->chrM\n",
    "hg38_url = \"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\"\n",
    "fa_gz    = os.path.join(REF_DIR, \"hg38.fa.gz\")\n",
    "fa_path  = os.path.join(REF_DIR, \"hg38.fa\")\n",
    "\n",
    "def _dl(url, dest):\n",
    "    if not os.path.exists(dest):\n",
    "        print(f\"Downloading {url} -> {dest}\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "    else:\n",
    "        print(f\"Exists: {dest}\")\n",
    "\n",
    "# Download and gunzip (first time only)\n",
    "_dl(hg38_url, fa_gz)\n",
    "if not os.path.exists(fa_path):\n",
    "    print(f\"Unzipping {fa_gz} -> {fa_path} (this can take a few minutes)\")\n",
    "    with gzip.open(fa_gz, \"rb\") as f_in, open(fa_path, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "else:\n",
    "    print(f\"Exists: {fa_path}\")\n",
    "\n",
    "print(\"Reference ready:\", os.path.exists(fa_path), \"| size (GB) ~\", round(os.path.getsize(fa_path)/1e9, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30ad5e-1d72-4b29-9f96-fad7824d91e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load ClinVar SNVs and Normalize Chrom Names (robust dtype handling)\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "src_csv  = os.path.join(DATA_DIR, \"clinvar_snvs_clean.csv\")\n",
    "assert os.path.exists(src_csv), f\"Missing: {src_csv} (run the VCF parsing step first).\"\n",
    "\n",
    "# 1) Read as strings to avoid mixed-type inference warnings, then coerce numerics\n",
    "df = pd.read_csv(\n",
    "    src_csv,\n",
    "    dtype=str,            # read everything as string first\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Clean up any accidental index column\n",
    "for idx_col in [\"Unnamed: 0\", \"index\"]:\n",
    "    if idx_col in df.columns:\n",
    "        df = df.drop(columns=[idx_col])\n",
    "\n",
    "# Strip column-name whitespace just in case\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# 2) Coerce numeric columns\n",
    "for col, dtype in [(\"POS\", \"Int64\"), (\"LABEL\", \"Int8\")]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(dtype)\n",
    "\n",
    "# 3) Normalize chromosome names to UCSC FASTA names\n",
    "def normalize_chr(chrom: str) -> str:\n",
    "    \"\"\"\n",
    "    Map ClinVar CHROM values to UCSC FASTA names:\n",
    "      '1' -> 'chr1', ..., '22' -> 'chr22', 'X' -> 'chrX', 'Y' -> 'chrY', 'MT' -> 'chrM'\n",
    "    If already 'chr*', leave as is.\n",
    "    \"\"\"\n",
    "    if chrom is None or pd.isna(chrom):\n",
    "        return chrom\n",
    "    c = str(chrom).strip()\n",
    "    if not c.startswith(\"chr\"):\n",
    "        c = \"chr\" + c\n",
    "    if c == \"chrMT\":\n",
    "        c = \"chrM\"\n",
    "    return c\n",
    "\n",
    "df[\"CHR_UCSC\"] = df[\"CHROM\"].apply(normalize_chr)\n",
    "\n",
    "print(df[[\"CHROM\", \"CHR_UCSC\"]].head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8d577-d583-45c8-a837-3b89c006cc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract REF/ALT windows (+- FLANK) with pyfaidx\n",
    "from pyfaidx import Fasta\n",
    "import numpy as np\n",
    "\n",
    "# configurable flank (±N bp)\n",
    "FLANK = 100  # try 50/100/200 as experiments\n",
    "MAX_ROWS = None  # e.g., set to 5000 for a quick test\n",
    "\n",
    "fa = Fasta(os.path.join(\"ref\",\"hg38.fa\"), as_raw=True, build_index=True)\n",
    "\n",
    "def fetch_windows(row, flank=FLANK):\n",
    "    \"\"\"\n",
    "    Returns (ref_window, alt_window) around POS for 1bp REF->ALT.\n",
    "    Performs a quick sanity check that reference base matches FASTA.\n",
    "    \"\"\"\n",
    "    chrom = row[\"CHR_UCSC\"]\n",
    "    pos   = int(row[\"POS\"])            # 1-based\n",
    "    ref   = str(row[\"REF\"])\n",
    "    alt   = str(row[\"ALT\"])\n",
    "\n",
    "    # window coordinates inclusive [start, end], 1-based for pyfaidx slicing\n",
    "    start = max(1, pos - flank)\n",
    "    end   = pos + flank\n",
    "\n",
    "    try:\n",
    "        window = fa[chrom][start:end]  # string slice from FASTA\n",
    "    except KeyError:\n",
    "        # chromosome absent in FASTA (shouldn't happen with UCSC hg38)\n",
    "        return None, None\n",
    "\n",
    "    window = str(window).upper()\n",
    "    # check the reference base at center (index relative to 'start')\n",
    "    center_idx = pos - start\n",
    "    if center_idx < 0 or center_idx >= len(window):\n",
    "        return None, None\n",
    "\n",
    "    # confirm the base matches expectation\n",
    "    if window[center_idx] != ref.upper():\n",
    "        # Sometimes reference mismatch can occur due to liftover/outdated positions.\n",
    "        # For now, skip mismatches to keep dataset clean.\n",
    "        return None, None\n",
    "\n",
    "    # build alt window by substituting at the center position\n",
    "    alt_window = window[:center_idx] + alt.upper() + window[center_idx+1:]\n",
    "\n",
    "    return window, alt_window\n",
    "\n",
    "# Apply (optionally on a subset for speed testing)\n",
    "work_df = df if MAX_ROWS is None else df.head(MAX_ROWS).copy()\n",
    "\n",
    "ref_windows, alt_windows = [], []\n",
    "skipped = 0\n",
    "\n",
    "for i, row in work_df.iterrows():\n",
    "    r, a = fetch_windows(row, FLANK)\n",
    "    if r is None or a is None:\n",
    "        ref_windows.append(np.nan)\n",
    "        alt_windows.append(np.nan)\n",
    "        skipped += 1\n",
    "    else:\n",
    "        ref_windows.append(r)\n",
    "        alt_windows.append(a)\n",
    "\n",
    "work_df[\"ref_seq\"] = ref_windows\n",
    "work_df[\"alt_seq\"] = alt_windows\n",
    "work_df = work_df.dropna(subset=[\"ref_seq\",\"alt_seq\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Built windows for {len(work_df)} variants; skipped {skipped} due to chromosome/position mismatches.\")\n",
    "print(work_df[[\"CHROM\",\"POS\",\"REF\",\"ALT\",\"LABEL\",\"ref_seq\",\"alt_seq\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cefc09-8025-46d0-ac23-0a310d6d38fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save ready-to-tokenize dataset\n",
    "out_csv = os.path.join(DATA_DIR, f\"clinvar_seq_pairs_flank{FLANK}.csv\")\n",
    "work_df.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv, \"| rows:\", len(work_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7cc993-79cb-45a8-a22c-d9c6ddd0bc40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Out of ~318 k ClinVar SNVs, ~80 k (25%) aligned perfectly to the reference genome (hg38.fa) after validation. The remaining variants likely represent assembly or coordinate mismatches.\n",
    "# I'm going to try NCBI's GRCh38 reference instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c1883-3c9f-44b7-a205-64c409758c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NCBI GRCh38 (RefSeq) — primary assembly FASTA\n",
    "# This file includes chromosomes and many scaffolds; we will auto-detect the\n",
    "# primary chromosomes by parsing headers ('chromosome 1', ..., 'X', 'Y', 'mitochondrion').\n",
    "\n",
    "import os, urllib.request, gzip, shutil, pathlib\n",
    "\n",
    "REF_DIR = os.path.join(\".\", \"ref\")\n",
    "os.makedirs(REF_DIR, exist_ok=True)\n",
    "\n",
    "# RefSeq assembly (p14 as of writing) — update URL if needed later\n",
    "NCBI_URL = (\"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/\"\n",
    "            \"GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz\")\n",
    "ncbi_gz   = os.path.join(REF_DIR, \"GRCh38.p14_genomic.fna.gz\")\n",
    "ncbi_fa   = os.path.join(REF_DIR, \"GRCh38.p14_genomic.fna\")\n",
    "\n",
    "def _dl(url, dest):\n",
    "    if not os.path.exists(dest):\n",
    "        print(f\"Downloading {url} -> {dest}\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "    else:\n",
    "        print(f\"Exists: {dest}\")\n",
    "\n",
    "_dl(NCBI_URL, ncbi_gz)\n",
    "\n",
    "if not os.path.exists(ncbi_fa):\n",
    "    print(f\"Unzipping {ncbi_gz} -> {ncbi_fa} (this can take a few minutes)\")\n",
    "    with gzip.open(ncbi_gz, \"rb\") as f_in, open(ncbi_fa, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "else:\n",
    "    print(f\"Exists: {ncbi_fa}\")\n",
    "\n",
    "print(\"NCBI FASTA ready:\", os.path.exists(ncbi_fa), \"| size (GB) ~\", round(os.path.getsize(ncbi_fa)/1e9, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b1de9-a968-472a-8071-2af66b3e3a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "fa_path = Path(ncbi_fa)\n",
    "assert fa_path.exists(), \"NCBI FASTA not found — run the previous cell first.\"\n",
    "\n",
    "# We'll scan headers to discover accessions and their chromosome names\n",
    "chrom_to_acc = {}   # e.g., {'1': 'NC_000001.11', '2': 'NC_000002.12', ..., 'MT': 'NC_012920.1'}\n",
    "\n",
    "want = {str(i) for i in range(1,23)} | {\"X\",\"Y\",\"MT\"}  # target set\n",
    "\n",
    "header_re = re.compile(r\"^>(\\S+).+?(chromosome\\s+([0-9XY]{1,2})|mitochondrion)\", re.IGNORECASE)\n",
    "\n",
    "with open(fa_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if not line.startswith(\">\"):\n",
    "            continue\n",
    "        m = header_re.search(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        acc = m.group(1)\n",
    "        chrom_label = m.group(3)\n",
    "        if chrom_label:\n",
    "            key = chrom_label.upper()\n",
    "        else:\n",
    "            # If 'mitochondrion' matched\n",
    "            key = \"MT\"\n",
    "        if key in want and key not in chrom_to_acc:\n",
    "            chrom_to_acc[key] = acc\n",
    "\n",
    "print(\"Discovered chromosome→accession mapping:\")\n",
    "for k in sorted(chrom_to_acc, key=lambda x: (x not in {\"X\",\"Y\",\"MT\"}, x if x.isdigit() else 100)):\n",
    "    print(f\"  {k:>2} -> {chrom_to_acc[k]}\")\n",
    "\n",
    "missing = want - set(chrom_to_acc.keys())\n",
    "if missing:\n",
    "    print(\"WARNING: Did not find entries for:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda4c594-50fa-435a-99a7-d9ec8308f7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chrom_to_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(chrom)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chrom_to_acc\u001b[38;5;241m.\u001b[39mget(key)\n\u001b[0;32m---> 22\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNCBI_SEQ\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCHROM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_ncbi_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHROM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNCBI_SEQ\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows with missing NCBI mapping:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNCBI_SEQ\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4943\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4809\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4810\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4815\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4816\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4817\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4818\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4819\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4934\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4937\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4941\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1422\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1502\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1502\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/base.py:925\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:2999\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mto_ncbi_name\u001b[0;34m(chrom)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(chrom)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchrom_to_acc\u001b[49m\u001b[38;5;241m.\u001b[39mget(key)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chrom_to_acc' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os\n",
    "\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "src_csv  = os.path.join(DATA_DIR, \"clinvar_snvs_clean.csv\")\n",
    "assert os.path.exists(src_csv), f\"Missing: {src_csv} (run the VCF parsing step first).\"\n",
    "\n",
    "df = pd.read_csv(src_csv, dtype=str)\n",
    "# Coerce numerics\n",
    "df[\"POS\"] = pd.to_numeric(df[\"POS\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"LABEL\"] = pd.to_numeric(df[\"LABEL\"], errors=\"coerce\").astype(\"Int8\")\n",
    "\n",
    "def to_ncbi_name(chrom: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Convert ClinVar CHROM ('1'..'22','X','Y','MT') to NCBI accession using parsed map.\n",
    "    Returns accession string like 'NC_000001.11' or None if unavailable.\n",
    "    \"\"\"\n",
    "    if chrom is None:\n",
    "        return None\n",
    "    key = str(chrom).strip().upper()\n",
    "    return chrom_to_acc.get(key)\n",
    "\n",
    "df[\"NCBI_SEQ\"] = df[\"CHROM\"].apply(to_ncbi_name)\n",
    "print(df[[\"CHROM\",\"NCBI_SEQ\"]].head())\n",
    "print(\"Rows with missing NCBI mapping:\", df[\"NCBI_SEQ\"].isna().sum())\n",
    "df = df.dropna(subset=[\"NCBI_SEQ\",\"POS\",\"REF\",\"ALT\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd4dfcd-3367-415b-be28-25f1e2c68087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fa_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m FLANK \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m     \u001b[38;5;66;03m# adjust (e.g., 50/100/200) as you wish\u001b[39;00m\n\u001b[1;32m      5\u001b[0m MAX_ROWS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# set to small int for quick test, None for all\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m fa_ncbi \u001b[38;5;241m=\u001b[39m Fasta(\u001b[38;5;28mstr\u001b[39m(\u001b[43mfa_path\u001b[49m), as_raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, build_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetch_windows_ncbi\u001b[39m(row, flank\u001b[38;5;241m=\u001b[39mFLANK):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Extract ref/alt windows from NCBI GRCh38 using accession names.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Validates that REF matches the FASTA at the center position.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fa_path' is not defined"
     ]
    }
   ],
   "source": [
    "from pyfaidx import Fasta\n",
    "import numpy as np\n",
    "\n",
    "FLANK = 100     # adjust (e.g., 50/100/200) as you wish\n",
    "MAX_ROWS = None # set to small int for quick test, None for all\n",
    "\n",
    "fa_ncbi = Fasta(str(fa_path), as_raw=True, build_index=True)\n",
    "\n",
    "def fetch_windows_ncbi(row, flank=FLANK):\n",
    "    \"\"\"\n",
    "    Extract ref/alt windows from NCBI GRCh38 using accession names.\n",
    "    Validates that REF matches the FASTA at the center position.\n",
    "    \"\"\"\n",
    "    acc = row[\"NCBI_SEQ\"]\n",
    "    pos = int(row[\"POS\"])   # 1-based\n",
    "    ref = str(row[\"REF\"]).upper()\n",
    "    alt = str(row[\"ALT\"]).upper()\n",
    "\n",
    "    start = max(1, pos - flank)\n",
    "    end   = pos + flank\n",
    "\n",
    "    try:\n",
    "        window = fa_ncbi[acc][start:end]\n",
    "    except KeyError:\n",
    "        return None, None\n",
    "\n",
    "    window = str(window).upper()\n",
    "    center_idx = pos - start\n",
    "    if center_idx < 0 or center_idx >= len(window):\n",
    "        return None, None\n",
    "\n",
    "    if window[center_idx] != ref:\n",
    "        return None, None\n",
    "\n",
    "    alt_window = window[:center_idx] + alt + window[center_idx+1:]\n",
    "    return window, alt_window\n",
    "\n",
    "work_df = df if MAX_ROWS is None else df.head(MAX_ROWS).copy()\n",
    "\n",
    "ref_windows, alt_windows = [], []\n",
    "skipped = 0\n",
    "for _, row in work_df.iterrows():\n",
    "    r, a = fetch_windows_ncbi(row, FLANK)\n",
    "    if r is None or a is None:\n",
    "        ref_windows.append(np.nan)\n",
    "        alt_windows.append(np.nan)\n",
    "        skipped += 1\n",
    "    else:\n",
    "        ref_windows.append(r)\n",
    "        alt_windows.append(a)\n",
    "\n",
    "work_df[\"ref_seq\"] = ref_windows\n",
    "work_df[\"alt_seq\"] = alt_windows\n",
    "work_df = work_df.dropna(subset=[\"ref_seq\", \"alt_seq\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Built windows for {len(work_df)} variants; skipped {skipped} due to reference mismatches or contigs.\")\n",
    "print(work_df[[\"CHROM\",\"NCBI_SEQ\",\"POS\",\"REF\",\"ALT\",\"LABEL\",\"ref_seq\",\"alt_seq\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440061f9-6081-4e04-8c8d-b27caa609b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_csv_ncbi = os.path.join(DATA_DIR, f\"clinvar_seq_pairs_ncbi_flank{FLANK}.csv\")\n",
    "work_df.to_csv(out_csv_ncbi, index=False)\n",
    "print(\"Saved:\", out_csv_ncbi, \"| rows:\", len(work_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85319786-2568-4ab3-b69b-c9d069a1c02d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 79825\n",
      "CHROM               object\n",
      "POS                  Int64\n",
      "REF                 object\n",
      "ALT                 object\n",
      "CLNSIG              object\n",
      "CLNREV              object\n",
      "GENEINFO            object\n",
      "LABEL                 Int8\n",
      "NCBI_SEQ            object\n",
      "ref_seq     string[python]\n",
      "alt_seq     string[python]\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>CLNSIG</th>\n",
       "      <th>CLNREV</th>\n",
       "      <th>GENEINFO</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>NCBI_SEQ</th>\n",
       "      <th>ref_seq</th>\n",
       "      <th>alt_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>930204</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>Benign</td>\n",
       "      <td>criteria_provided,_multiple_submitters,_no_con...</td>\n",
       "      <td>SAMD11:148398</td>\n",
       "      <td>0</td>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...</td>\n",
       "      <td>CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>930248</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>Likely_benign</td>\n",
       "      <td>criteria_provided,_multiple_submitters,_no_con...</td>\n",
       "      <td>SAMD11:148398</td>\n",
       "      <td>0</td>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...</td>\n",
       "      <td>CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM     POS REF ALT         CLNSIG  \\\n",
       "0     1  930204   G   A         Benign   \n",
       "1     1  930248   G   A  Likely_benign   \n",
       "\n",
       "                                              CLNREV       GENEINFO  LABEL  \\\n",
       "0  criteria_provided,_multiple_submitters,_no_con...  SAMD11:148398      0   \n",
       "1  criteria_provided,_multiple_submitters,_no_con...  SAMD11:148398      0   \n",
       "\n",
       "       NCBI_SEQ                                            ref_seq  \\\n",
       "0  NC_000001.11  CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...   \n",
       "1  NC_000001.11  CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...   \n",
       "\n",
       "                                             alt_seq  \n",
       "0  CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCA...  \n",
       "1  CACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGC...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for DNABERT fine-tuning\n",
    "# Config and Load sequence pairs\n",
    "import os, pandas as pd\n",
    "\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "FLANK = 100\n",
    "K = 6\n",
    "\n",
    "src_csv = os.path.join(DATA_DIR, f\"clinvar_seq_pairs_ncbi_flank{FLANK}.csv\")\n",
    "assert os.path.exists(src_csv), f\"Missing: {src_csv}\"\n",
    "\n",
    "# Read everything as string first; prevents DtypeWarning\n",
    "df = pd.read_csv(src_csv, dtype=str, low_memory=False)\n",
    "\n",
    "# Clean stray index cols if present\n",
    "for c in (\"Unnamed: 0\", \"index\"):\n",
    "    if c in df.columns:\n",
    "        df = df.drop(columns=[c])\n",
    "\n",
    "# Coerce numeric columns (nullable dtypes keep NaN if any)\n",
    "if \"POS\" in df.columns:\n",
    "    df[\"POS\"] = pd.to_numeric(df[\"POS\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"LABEL\" in df.columns:\n",
    "    df[\"LABEL\"] = pd.to_numeric(df[\"LABEL\"], errors=\"coerce\").astype(\"Int8\")\n",
    "\n",
    "# Make sure sequence/text columns are strings\n",
    "for c in (\"ref_seq\", \"alt_seq\"):\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"string\")\n",
    "\n",
    "df = df.dropna(subset=[\"alt_seq\", \"ref_seq\", \"LABEL\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(df.dtypes)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2923bec6-c522-44c9-99e9-d4d1d24bcdc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGCCTGCCGCCCGGAACCTGAAGAAGGAGCGAACTCCCAGCTTCTCTGCCAGCGATGGTGACAGCGACGGGAGTGGCCCCACCTGTGGGCGGCGGCCAGGCTTGAAGCAGGAG\n",
      "CCCACCTTCCTCTCCTCCTGCCCCACCTTCCTCTCCTCCTGCCCCACCAGAACCGGGGGCGGCTGGCAGACAAGAGGACAGTCGCCCTGCCTGCCGCCCGAAACCTGAAGAAGGAGCGAACTCCCAGCTTCTCTGCCAGCGATGGTGACAGCGACGGGAGTGGCCCCACCTGTGGGCGGCGGCCAGGCTTGAAGCAGGAG\n"
     ]
    }
   ],
   "source": [
    "# Verifying no whitespace in sequences. Just Jupyter formatting\n",
    "print(df[\"ref_seq\"].iloc[0])\n",
    "print(df[\"alt_seq\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "326818ba-bc8c-43da-b7d9-7dea0aa4312f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k-mer Utilities\n",
    "import re\n",
    "\n",
    "VALID = set(\"ACGT\")\n",
    "\n",
    "def clean_seq(seq: str) -> str:\n",
    "    \"\"\"Uppercase and replace any non-ACGT with 'A' (simple, fast).\"\"\"\n",
    "    s = str(seq).upper()\n",
    "    return \"\".join(ch if ch in VALID else \"A\" for ch in s)\n",
    "\n",
    "def kmerize(seq: str, k: int = 6) -> str:\n",
    "    \"\"\"Return space-separated overlapping k-mers (DNABERT input).\"\"\"\n",
    "    s = clean_seq(seq)\n",
    "    if len(s) < k:\n",
    "        return \"\"  # will be dropped later\n",
    "    return \" \".join(s[i:i+k] for i in range(len(s) - k + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091965a6-941f-4ec0-929a-a1e1330b9214",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 rows with too-short sequences for k=6.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>text_alt_k6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>930204</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>CCCACC CCACCT CACCTT ACCTTC CCTTCC CTTCCT TTCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>930248</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>CACCAG ACCAGA CCAGAA CAGAAC AGAACC GAACCG AACC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM     POS REF ALT  LABEL  \\\n",
       "0     1  930204   G   A      0   \n",
       "1     1  930248   G   A      0   \n",
       "\n",
       "                                         text_alt_k6  \n",
       "0  CCCACC CCACCT CACCTT ACCTTC CCTTCC CTTCCT TTCC...  \n",
       "1  CACCAG ACCAGA CCAGAA CAGAAC AGAACC GAACCG AACC...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Built DNABERT text columns (ALT Baseline + optional REF)\n",
    "# ALT-only baseline (recommended first experiment)\n",
    "df[\"text_alt_k6\"] = df[\"alt_seq\"].map(lambda s: kmerize(s, k=K))\n",
    "\n",
    "# (Optional) REF text as well, if you plan paired experiments later\n",
    "df[\"text_ref_k6\"] = df[\"ref_seq\"].map(lambda s: kmerize(s, k=K))\n",
    "\n",
    "# Drop any rows where k-merization failed (too short, etc.)\n",
    "before = len(df)\n",
    "df = df[(df[\"text_alt_k6\"].str.len() > 0)]\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} rows with too-short sequences for k={K}.\")\n",
    "df[[\"CHROM\",\"POS\",\"REF\",\"ALT\",\"LABEL\",\"text_alt_k6\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd8b8880-4ed1-4238-bc41-880a2198dadb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "   ./data/prepared_k6_alt_only_all.csv (rows=79825)\n",
      "   ./data/prepared_k6_alt_only_train.csv (rows=63860)\n",
      "   ./data/prepared_k6_alt_only_val.csv (rows=15965)\n",
      "Label balance (all):\n",
      " LABEL\n",
      "0    0.836\n",
      "1    0.164\n",
      "Name: proportion, dtype: Float64\n"
     ]
    }
   ],
   "source": [
    "# Stratified train/val split and save CSVs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "keep_cols = [\"CHROM\",\"POS\",\"REF\",\"ALT\",\"LABEL\",\"text_alt_k6\",\"text_ref_k6\",\"ref_seq\",\"alt_seq\"]\n",
    "prepared = df[keep_cols].copy()\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    prepared,\n",
    "    test_size=0.2,\n",
    "    stratify=prepared[\"LABEL\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "prep_all  = os.path.join(DATA_DIR, f\"prepared_k{K}_alt_only_all.csv\")\n",
    "prep_tr   = os.path.join(DATA_DIR, f\"prepared_k{K}_alt_only_train.csv\")\n",
    "prep_val  = os.path.join(DATA_DIR, f\"prepared_k{K}_alt_only_val.csv\")\n",
    "\n",
    "prepared.to_csv(prep_all, index=False)\n",
    "train_df.to_csv(prep_tr, index=False)\n",
    "val_df.to_csv(prep_val, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"  \", prep_all, f\"(rows={len(prepared)})\")\n",
    "print(\"  \", prep_tr,  f\"(rows={len(train_df)})\")\n",
    "print(\"  \", prep_val, f\"(rows={len(val_df)})\")\n",
    "print(\"Label balance (all):\\n\", prepared[\"LABEL\"].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26b7d448-73e0-4dee-a3f4-d53596b12411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c133c0552f8d4c8b86c5566cf11713a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hugging Face Login\n",
    "# Only needed if your environment blocks anonymous HF downloads\n",
    "# Get a token from https://huggingface.co/settings/tokens\n",
    "# and paste it when prompted.\n",
    "from huggingface_hub import login\n",
    "try:\n",
    "    login()  # or login(token=\"hf_...\") to pass programmatically\n",
    "except Exception as e:\n",
    "    print(\"HF login skipped or not required:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff12c723-b389-4f5e-8bf0-2ff68bbc4398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9084d934d44a1db44d9259cfb28b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825367daa6f644a0bfeec08718724a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fc794fa5414c5c8e0f1bbbec4d8935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e622922f5c49d3a02e675ee3ab205e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from: zhihan1996/DNA_bert_6\n",
      "Tokenized input_ids shape: torch.Size([1, 512])\n",
      "First 10 tokens: [2, 2703, 2606, 2218, 667, 2655, 2414, 1451, 1694, 2667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load DNABERT tokenizer \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_CANDIDATES = [\n",
    "    \"zhihan1996/DNA_bert_6\",     # official DNABERT (k=6)\n",
    "    \"armheb/DNA_bert_6\",         # public mirror\n",
    "    # add more fallbacks here if needed\n",
    "]\n",
    "\n",
    "tok = None\n",
    "last_err = None\n",
    "for mid in MODEL_CANDIDATES:\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(mid, use_fast=False)  # DNABERT uses a classic BertTokenizer\n",
    "        MODEL_NAME = mid\n",
    "        print(\"Loaded tokenizer from:\", mid)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "        print(f\"Failed to load {mid}: {e}\")\n",
    "\n",
    "if tok is None:\n",
    "    raise last_err\n",
    "\n",
    "# Your existing sanity check\n",
    "sample = prepared[\"text_alt_k6\"].iloc[0]\n",
    "enc = tok(sample, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "print(\"Tokenized input_ids shape:\", enc[\"input_ids\"].shape)\n",
    "print(\"First 10 tokens:\", enc[\"input_ids\"][0, :10].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0b3e4-b5ca-4ce1-aff4-103ba9ae0555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now let's train. \n",
    "# 1. Baseline full fine-tune\n",
    "# 2. Weighted Loss (to handle the 83.6/16.4 class imbalance. May address this later\n",
    "# 3. LoRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249e1d4-04f3-4897-927c-a099346fe87f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One-time fixing packages\n",
    "#%pip install -U \"transformers>=4.44.2\" \"accelerate>=0.34.2\" \"peft>=0.13.2\" \"datasets>=2.20\"\n",
    "\n",
    "#import transformers, torch\n",
    "#print(\"Transformers:\", transformers.__version__)\n",
    "#print(\"Torch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ea195-4524-4d8f-89ff-d92733df7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time fix for transformers version\n",
    "\n",
    "#%pip uninstall -y transformers\n",
    "#%pip install \"transformers==4.44.2\"  # known-good, definitely has evaluation_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81918894-6ea8-4b64-b31f-d60403fb1d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import inspect\n",
    "\"evaluation_strategy\" in inspect.signature(TrainingArguments.__init__).parameters\n",
    "# should print: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3531276-67c2-4087-af11-821624158d58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNA_bert_6 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7983' max='7983' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7983/7983 34:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.380500</td>\n",
       "      <td>0.369016</td>\n",
       "      <td>0.797503</td>\n",
       "      <td>0.852928</td>\n",
       "      <td>0.612745</td>\n",
       "      <td>0.285823</td>\n",
       "      <td>0.389813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.7975033924852966,\n",
       " 'accuracy': 0.8529282806138427,\n",
       " 'precision': 0.6127450980392157,\n",
       " 'recall': 0.2858231707317073,\n",
       " 'f1': 0.3898128898128898}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Baseline full fine-tune\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "MODEL_NAME = mid  # from the tokenizer cell you just ran\n",
    "tokenizer = tok\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "class KmerTextDataset(Dataset):\n",
    "    def __init__(self, df, text_col=\"text_alt_k6\", label_col=\"LABEL\", max_length=512):\n",
    "        self.texts = df[text_col].tolist()\n",
    "        self.labels = df[label_col].astype(int).tolist()\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = KmerTextDataset(train_df)\n",
    "val_ds   = KmerTextDataset(val_df)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def compute_classification_metrics(y_true, y_prob):\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    return {\"auc\": auc, \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "def hf_compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1)[:,1].numpy()\n",
    "    return compute_classification_metrics(labels, probs)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/dnabert_fullft\",\n",
    "    num_train_epochs=1,                 # start with 1 for smoke test; then 3–5\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auc\",        # since your compute_metrics returns 'auc'\n",
    "    greater_is_better=True,\n",
    "    report_to=[],                       # avoids wandb/autologging surprises\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=hf_compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()   # <-- run when ready\n",
    "preds = trainer.predict(val_ds)\n",
    "y_prob = torch.softmax(torch.tensor(preds.predictions), dim=1)[:,1].numpy()\n",
    "metrics = compute_classification_metrics(preds.label_ids, y_prob); metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe2f4bec-fc0a-4fa8-abe8-c6e5fd27f51a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39915' max='39915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39915/39915 2:53:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.364100</td>\n",
       "      <td>0.355586</td>\n",
       "      <td>0.836388</td>\n",
       "      <td>0.857626</td>\n",
       "      <td>0.584497</td>\n",
       "      <td>0.462652</td>\n",
       "      <td>0.516486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.332220</td>\n",
       "      <td>0.871593</td>\n",
       "      <td>0.879173</td>\n",
       "      <td>0.713059</td>\n",
       "      <td>0.443216</td>\n",
       "      <td>0.546651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.432063</td>\n",
       "      <td>0.880534</td>\n",
       "      <td>0.884435</td>\n",
       "      <td>0.712958</td>\n",
       "      <td>0.496951</td>\n",
       "      <td>0.585673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.464837</td>\n",
       "      <td>0.881413</td>\n",
       "      <td>0.886502</td>\n",
       "      <td>0.703407</td>\n",
       "      <td>0.535061</td>\n",
       "      <td>0.607792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.532064</td>\n",
       "      <td>0.878959</td>\n",
       "      <td>0.886564</td>\n",
       "      <td>0.693295</td>\n",
       "      <td>0.555640</td>\n",
       "      <td>0.616882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>eval_auc</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836388</td>\n",
       "      <td>0.516486</td>\n",
       "      <td>0.857626</td>\n",
       "      <td>0.584497</td>\n",
       "      <td>0.462652</td>\n",
       "      <td>0.355586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.871593</td>\n",
       "      <td>0.546651</td>\n",
       "      <td>0.879173</td>\n",
       "      <td>0.713059</td>\n",
       "      <td>0.443216</td>\n",
       "      <td>0.332220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.880534</td>\n",
       "      <td>0.585673</td>\n",
       "      <td>0.884435</td>\n",
       "      <td>0.712958</td>\n",
       "      <td>0.496951</td>\n",
       "      <td>0.432063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.881413</td>\n",
       "      <td>0.607792</td>\n",
       "      <td>0.886502</td>\n",
       "      <td>0.703407</td>\n",
       "      <td>0.535061</td>\n",
       "      <td>0.464837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.878959</td>\n",
       "      <td>0.616882</td>\n",
       "      <td>0.886564</td>\n",
       "      <td>0.693295</td>\n",
       "      <td>0.555640</td>\n",
       "      <td>0.532064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  eval_auc   eval_f1  eval_accuracy  eval_precision  eval_recall  \\\n",
       "0    1.0  0.836388  0.516486       0.857626        0.584497     0.462652   \n",
       "1    2.0  0.871593  0.546651       0.879173        0.713059     0.443216   \n",
       "2    3.0  0.880534  0.585673       0.884435        0.712958     0.496951   \n",
       "3    4.0  0.881413  0.607792       0.886502        0.703407     0.535061   \n",
       "4    5.0  0.878959  0.616882       0.886564        0.693295     0.555640   \n",
       "\n",
       "   eval_loss  \n",
       "0   0.355586  \n",
       "1   0.332220  \n",
       "2   0.432063  \n",
       "3   0.464837  \n",
       "4   0.532064  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-checkpoint metrics: {'auc': 0.8814127141756296, 'accuracy': 0.8865017225180082, 'precision': 0.7034068136272545, 'recall': 0.5350609756097561, 'f1': 0.6077922077922078}\n",
      "Saved per-epoch metrics -> ./runs/dnabert_fullft_es/val_metrics_by_epoch.csv\n"
     ]
    }
   ],
   "source": [
    "# === Early-stopping training run (max 5 epochs) ===\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, TrainerCallback\n",
    "import torch, pandas as pd\n",
    "\n",
    "# Callback to capture per-epoch metrics cleanly\n",
    "class MetricHistory(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.rows = []\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is None: \n",
    "            return\n",
    "        row = {\"epoch\": state.epoch}\n",
    "        # keep the common ones if present\n",
    "        for k in [\"eval_auc\",\"eval_f1\",\"eval_accuracy\",\"eval_precision\",\"eval_recall\",\"eval_loss\"]:\n",
    "            if k in metrics: row[k] = metrics[k]\n",
    "        self.rows.append(row)\n",
    "\n",
    "hist = MetricHistory()\n",
    "\n",
    "training_args_es = TrainingArguments(\n",
    "    output_dir=\"./runs/dnabert_fullft_es\",\n",
    "    num_train_epochs=5,                 # upper bound\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auc\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=100,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer_es = Trainer(\n",
    "    model=model,                        # continues from your current model (after 1 epoch)\n",
    "    args=training_args_es,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=hf_compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=2,          # stop after 2 epochs w/o sufficient AUC gain\n",
    "            early_stopping_threshold=0.002      # min AUC improvement to count as “better”\n",
    "        ),\n",
    "        hist\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "train_output = trainer_es.train()\n",
    "\n",
    "# Show per-epoch validation metrics\n",
    "df_hist = pd.DataFrame(hist.rows).sort_values(\"epoch\")\n",
    "display(df_hist)\n",
    "\n",
    "# Evaluate the best checkpoint on the validation set\n",
    "preds_es = trainer_es.predict(val_ds)\n",
    "y_prob_es = torch.softmax(torch.tensor(preds_es.predictions), dim=1)[:,1].numpy()\n",
    "metrics_es = compute_classification_metrics(preds_es.label_ids, y_prob_es)\n",
    "print(\"Best-checkpoint metrics:\", metrics_es)\n",
    "\n",
    "# (Optional) save the history for your report\n",
    "df_hist.to_csv(\"./runs/dnabert_fullft_es/val_metrics_by_epoch.csv\", index=False)\n",
    "print(\"Saved per-epoch metrics -> ./runs/dnabert_fullft_es/val_metrics_by_epoch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f64206b-3595-4b5c-811a-dc16d4ecfd34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved best model from Epoch 4 → ./runs/dnabert_fullft_es/best_model\n"
     ]
    }
   ],
   "source": [
    "# Save Epoch 4 or model with best AUC due to class imbalance \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "best_ckpt = \"./runs/dnabert_fullft_es/checkpoint-31932\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_ckpt)\n",
    "\n",
    "save_dir = \"./runs/dnabert_fullft_es/best_model\"\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"✅ Saved best model from Epoch 4 → {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15817e-2ed1-4704-bc12-d3fdb47a8ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
