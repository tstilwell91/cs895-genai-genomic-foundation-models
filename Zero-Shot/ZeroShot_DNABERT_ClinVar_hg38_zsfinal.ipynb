{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a00e623",
   "metadata": {},
   "source": [
    "\n",
    "# Zero‑Shot DNABERT on ClinVar (Ref vs Alt) — **hg38**\n",
    "This notebook:\n",
    "1. Uses or downloads **ClinVar GRCh38 VCF** (from NCBI).\n",
    "2. Downloads **UCSC hg38** FASTA (`chr1`, `chr2`, …) and indexes it.\n",
    "3. Builds **ref+alt** sequence windows (±100bp) for biallelic SNVs with clear **CLNSIG** (P/LP vs B/LB).\n",
    "4. Runs **zero‑shot DNABERT (v1 6‑mer)** on ref vs alt and reports AUROC/AUPRC + Accuracy/F1/Precision/Recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f28e9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Configuration ===\n",
    "import os\n",
    "\n",
    "DATA_DIR = os.path.expanduser(\"/home/tstil004/phd/cs895_genai/data\")    # read/write\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "VAL_CSV  = f\"{DATA_DIR}/clinvar_seq_pairs_hg38_flank100.csv\"\n",
    "\n",
    "CLINVAR_VCF_URL = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz\"\n",
    "UCSC_HG38_FASTA_URL = \"http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/latest/hg38.fa.gz\"\n",
    "\n",
    "VCF_GZ  = f\"{DATA_DIR}/clinvar.vcf.gz\"\n",
    "VCF_TBI = f\"{DATA_DIR}/clinvar.vcf.gz.tbi\"\n",
    "FA_GZ   = f\"{DATA_DIR}/hg38.fa.gz\"\n",
    "FA      = f\"{DATA_DIR}/hg38.fa\"\n",
    "\n",
    "FLANK   = 100\n",
    "CHROM_PREFIX = \"chr\"     # Map '1' -> 'chr1' for UCSC hg38\n",
    "SEED = 42\n",
    "\n",
    "MODEL_NAME = \"zhihan1996/DNA_bert_6\"\n",
    "KMER = 6\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(\"Config OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d28fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install --quiet transformers==4.44.2 torch==2.4.0 pyfaidx==0.7.2.1 pandas==2.1.4 scikit-learn==1.3.2 numpy==1.26.4 tqdm==4.66.4 pysam==0.22.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ce5931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tstil004/envs/dnabert/lib/python3.10/site-packages/pyfaidx/__init__.py:21: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils OK.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, gzip, shutil, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "from pyfaidx import Fasta\n",
    "import pysam\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def maybe_download(url: str, dest: str):\n",
    "    if not os.path.exists(dest):\n",
    "        import urllib.request\n",
    "        print(f\"Downloading {url} -> {dest}\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "    else:\n",
    "        print(f\"Found existing: {dest}\")\n",
    "\n",
    "def gunzip_if_needed(src_gz: str, dst: str):\n",
    "    if not os.path.exists(dst) and os.path.exists(src_gz):\n",
    "        print(f\"Decompressing {src_gz} -> {dst}\")\n",
    "        with gzip.open(src_gz, 'rb') as f_in, open(dst, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    else:\n",
    "        if os.path.exists(dst):\n",
    "            print(f\"Found existing: {dst}\")\n",
    "        else:\n",
    "            print(f\"Missing source to gunzip: {src_gz}\")\n",
    "\n",
    "def index_fasta_if_needed(fa_path: str):\n",
    "    fai = fa_path + \".fai\"\n",
    "    if not os.path.exists(fai):\n",
    "        print(f\"Indexing FASTA: {fa_path}\")\n",
    "        _ = Fasta(fa_path, as_raw=True)  # creates .fai\n",
    "        _.close()\n",
    "    else:\n",
    "        print(f\"Found FASTA index: {fai}\")\n",
    "\n",
    "def detect_csv_pairs(path:str) -> bool:\n",
    "    return os.path.exists(path) and os.path.getsize(path) > 0\n",
    "\n",
    "print(\"Utils OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508cb4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has ref+alt pairs CSV: False | /home/tstil004/phd/cs895_genai/data/clinvar_seq_pairs_hg38_flank100.csv\n",
      "No ref+alt pairs file found. Preparing inputs...\n",
      "Found existing: /home/tstil004/phd/cs895_genai/data/clinvar.vcf.gz\n",
      "Found existing: /home/tstil004/phd/cs895_genai/data/clinvar.vcf.gz.tbi\n",
      "Downloading http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/latest/hg38.fa.gz -> /home/tstil004/phd/cs895_genai/data/hg38.fa.gz\n",
      "Decompressing /home/tstil004/phd/cs895_genai/data/hg38.fa.gz -> /home/tstil004/phd/cs895_genai/data/hg38.fa\n",
      "Indexing FASTA: /home/tstil004/phd/cs895_genai/data/hg38.fa\n",
      "Found VCF index: /home/tstil004/phd/cs895_genai/data/clinvar.vcf.gz.tbi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "USE_EXISTING_PAIRS = detect_csv_pairs(VAL_CSV)\n",
    "print(\"Has ref+alt pairs CSV:\", USE_EXISTING_PAIRS, \"|\", VAL_CSV)\n",
    "\n",
    "if not USE_EXISTING_PAIRS:\n",
    "    print(\"No ref+alt pairs file found. Preparing inputs...\")\n",
    "    maybe_download(CLINVAR_VCF_URL, VCF_GZ)\n",
    "    try:\n",
    "        maybe_download(CLINVAR_VCF_URL + \".tbi\", VCF_TBI)\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't fetch prebuilt .tbi; will tabix later:\", e)\n",
    "\n",
    "    maybe_download(UCSC_HG38_FASTA_URL, FA_GZ)\n",
    "    gunzip_if_needed(FA_GZ, FA)\n",
    "    index_fasta_if_needed(FA)\n",
    "\n",
    "    if not os.path.exists(VCF_TBI):\n",
    "        print(\"Tabix-indexing ClinVar VCF...\")\n",
    "        pysam.tabix_index(VCF_GZ, preset=\"vcf\", force=True)\n",
    "    else:\n",
    "        print(\"Found VCF index:\", VCF_TBI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f82e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning VCF for SNVs...\n",
      "Writing pairs: 1531207 -> /home/tstil004/phd/cs895_genai/data/clinvar_seq_pairs_hg38_flank100.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>ref_seq</th>\n",
       "      <th>alt_seq</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>69134</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>AGGTAACTGCAGAGGCTATTTCCTGGAATGAATCAACGAGTGAAAC...</td>\n",
       "      <td>AGGTAACTGCAGAGGCTATTTCCTGGAATGAATCAACGAGTGAAAC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>924518</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>CCACCGGGGCGCCATGCCGGCGGTCAAGAAGGAGTTCCCGGGCCGC...</td>\n",
       "      <td>CCACCGGGGCGCCATGCCGGCGGTCAAGAAGGAGTTCCCGGGCCGC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>925956</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>CTGCCGCTGACTGCGCGCAGAAGCGTGCCGCTCCCTCACAGGGTCT...</td>\n",
       "      <td>CTGCCGCTGACTGCGCGCAGAAGCGTGCCGCTCCCTCACAGGGTCT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>925969</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>CGCGCAGAAGCGTGCCGCTCCCTCACAGGGTCTGCCTCGGCTCTGC...</td>\n",
       "      <td>CGCGCAGAAGCGTGCCGCTCCCTCACAGGGTCTGCCTCGGCTCTGC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>925980</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>GTGCCGCTCCCTCACAGGGTCTGCCTCGGCTCTGCTCGCAGGGAAA...</td>\n",
       "      <td>GTGCCGCTCCCTCACAGGGTCTGCCTCGGCTCTGCTCGCAGGGAAA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM     POS REF ALT                                            ref_seq  \\\n",
       "0     1   69134   A   G  AGGTAACTGCAGAGGCTATTTCCTGGAATGAATCAACGAGTGAAAC...   \n",
       "1     1  924518   G   C  CCACCGGGGCGCCATGCCGGCGGTCAAGAAGGAGTTCCCGGGCCGC...   \n",
       "2     1  925956   C   T  CTGCCGCTGACTGCGCGCAGAAGCGTGCCGCTCCCTCACAGGGTCT...   \n",
       "3     1  925969   C   T  CGCGCAGAAGCGTGCCGCTCCCTCACAGGGTCTGCCTCGGCTCTGC...   \n",
       "4     1  925980   C   T  GTGCCGCTCCCTCACAGGGTCTGCCTCGGCTCTGCTCGCAGGGAAA...   \n",
       "\n",
       "                                             alt_seq  label  \n",
       "0  AGGTAACTGCAGAGGCTATTTCCTGGAATGAATCAACGAGTGAAAC...      0  \n",
       "1  CCACCGGGGCGCCATGCCGGCGGTCAAGAAGGAGTTCCCGGGCCGC...      0  \n",
       "2  CTGCCGCTGACTGCGCGCAGAAGCGTGCCGCTCCCTCACAGGGTCT...      0  \n",
       "3  CGCGCAGAAGCGTGCCGCTCCCTCACAGGGTCTGCCTCGGCTCTGC...      0  \n",
       "4  GTGCCGCTCCCTCACAGGGTCTGCCTCGGCTCTGCTCGCAGGGAAA...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _norm_base(s):\n",
    "    return str(s).upper().replace(\"U\", \"T\")\n",
    "\n",
    "def _chrom_with_prefix(c, prefix):\n",
    "    c = str(c)\n",
    "    if prefix and not c.startswith(prefix):\n",
    "        return prefix + c\n",
    "    return c\n",
    "\n",
    "def _safe_parse_vcf_line(rec):\n",
    "    # pysam will yield BYTES when encoding=None; handle both types safely\n",
    "    if isinstance(rec, bytes):\n",
    "        # decode as utf-8; ignore any stray non-utf8 bytes\n",
    "        rec = rec.decode(\"utf-8\", errors=\"ignore\")\n",
    "    rec = rec.strip()\n",
    "    if not rec or rec.startswith(\"#\"):\n",
    "        return None\n",
    "    parts = rec.split(\"\\t\")  # REAL tab\n",
    "    if len(parts) < 8:\n",
    "        return None\n",
    "    chrom, pos, _id, ref, alt, qual, flt, info = parts[:8]\n",
    "    return chrom, pos, _id, ref, alt, qual, flt, info\n",
    "\n",
    "def build_ref_alt_pairs_from_vcf(\n",
    "    vcf_gz: str,\n",
    "    fasta_path: str,\n",
    "    out_csv: str,\n",
    "    flank: int = 100,\n",
    "    chrom_prefix: str = \"chr\",\n",
    "    max_records: Optional[int] = None,\n",
    "):\n",
    "    # Open FASTA and VCF\n",
    "    fa = Fasta(fasta_path, as_raw=True, sequence_always_upper=True)\n",
    "    # CRITICAL: disable pysam's internal ascii decoding\n",
    "    tbx = pysam.TabixFile(vcf_gz, encoding=None)\n",
    "\n",
    "    rows, kept = [], 0\n",
    "    print(\"Scanning VCF for SNVs...\")\n",
    "\n",
    "    for rec in tbx.fetch():\n",
    "        parsed = _safe_parse_vcf_line(rec)\n",
    "        if parsed is None:\n",
    "            continue\n",
    "        chrom, pos, _id, ref, alt, qual, flt, info = parsed\n",
    "        try:\n",
    "            pos = int(pos)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        ref = _norm_base(ref)\n",
    "        alt = _norm_base(alt)\n",
    "\n",
    "        # only biallelic 1bp SNVs\n",
    "        if len(ref) != 1 or \",\" in alt or len(alt) != 1:\n",
    "            continue\n",
    "\n",
    "        # CLNSIG -> binary label\n",
    "        label = None\n",
    "        for field in info.split(\";\"):\n",
    "            if field.startswith(\"CLNSIG=\"):\n",
    "                sig = field.split(\"=\", 1)[1].lower()\n",
    "                has_path = (\"pathogenic\" in sig)\n",
    "                has_ben  = (\"benign\" in sig)\n",
    "                if has_path and not has_ben:\n",
    "                    label = 1\n",
    "                elif has_ben and not has_path:\n",
    "                    label = 0\n",
    "                break\n",
    "        if label is None:\n",
    "            continue\n",
    "\n",
    "        c = _chrom_with_prefix(chrom, chrom_prefix)  # e.g., '1' -> 'chr1' for hg38\n",
    "        start = max(1, pos - flank)\n",
    "        end   = pos + flank\n",
    "\n",
    "        try:\n",
    "            ctx = str(fa[c][start:end])\n",
    "        except KeyError:\n",
    "            # contig naming mismatch, skip\n",
    "            continue\n",
    "        if len(ctx) != (end - start):\n",
    "            continue\n",
    "\n",
    "        center  = pos - start\n",
    "        ref_seq = ctx\n",
    "        alt_seq = ctx[:center] + alt + ctx[center+1:]\n",
    "\n",
    "        rows.append((chrom, pos, ref, alt, ref_seq, alt_seq, label))\n",
    "        kept += 1\n",
    "        if max_records and kept >= max_records:\n",
    "            break\n",
    "\n",
    "    print(f\"Writing pairs: {len(rows)} -> {out_csv}\")\n",
    "    df = pd.DataFrame(rows, columns=[\"CHROM\", \"POS\", \"REF\", \"ALT\", \"ref_seq\", \"alt_seq\", \"label\"])\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    return out_csv\n",
    "\n",
    "# Build if needed\n",
    "if not USE_EXISTING_PAIRS:\n",
    "    build_ref_alt_pairs_from_vcf(VCF_GZ, FA, VAL_CSV, flank=FLANK, chrom_prefix=CHROM_PREFIX, max_records=None)\n",
    "else:\n",
    "    print(\"Using existing:\", VAL_CSV)\n",
    "\n",
    "df = pd.read_csv(VAL_CSV, dtype={\"CHROM\": str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcefc3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1531207/1531207 rows with binary labels.\n",
      "(918724, 7) (306241, 7) (306242, 7)\n",
      "Label balance (val):  {0: 0.7888884897841896, 1: 0.21111151021581043}\n",
      "Label balance (test): {0: 0.7888891791459042, 1: 0.21111082085409577}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Labels + split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "if \"label\" not in df.columns:\n",
    "    if \"LABEL\" in df.columns:\n",
    "        df[\"label\"] = df[\"LABEL\"].astype(int)\n",
    "    elif \"CLNSIG\" in df.columns:\n",
    "        def clinsig_to_label(v):\n",
    "            s = str(v).lower()\n",
    "            has_path = (\"pathogenic\" in s); has_ben = (\"benign\" in s)\n",
    "            if has_path and not has_ben: return 1\n",
    "            if has_ben  and not has_path: return 0\n",
    "            return np.nan\n",
    "        df[\"label\"] = df[\"CLNSIG\"].apply(clinsig_to_label).astype(\"float\")\n",
    "    else:\n",
    "        raise KeyError(\"No label/LABEL/CLNSIG in dataframe.\")\n",
    "\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[\"label\"]).copy()\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "after = len(df)\n",
    "print(f\"Using {after}/{before} rows with binary labels.\")\n",
    "assert {\"ref_seq\",\"alt_seq\"}.issubset(df.columns)\n",
    "\n",
    "df_train, df_tmp = train_test_split(df, test_size=0.4, random_state=SEED, stratify=df[\"label\"])\n",
    "df_val, df_test  = train_test_split(df_tmp, test_size=0.5, random_state=SEED, stratify=df_tmp[\"label\"])\n",
    "print(df_train.shape, df_val.shape, df_test.shape)\n",
    "print(\"Label balance (val): \", df_val['label'].value_counts(normalize=True).to_dict())\n",
    "print(\"Label balance (test):\", df_test['label'].value_counts(normalize=True).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ab0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Zero-shot DNABERT (v1)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertConfig, BertTokenizerFast\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"zhihan1996/DNA_bert_6\"\n",
    "KMER = 6; MAX_LEN = 512; BATCH_SIZE = 32\n",
    "\n",
    "class SNVDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        return {\"ref_seq\": r[\"ref_seq\"], \"alt_seq\": r[\"alt_seq\"], \"labels\": int(r[\"label\"])}\n",
    "\n",
    "def _kmerize(seq: str, k: int) -> str:\n",
    "    seq = str(seq).strip().upper().replace(\"U\",\"T\")\n",
    "    if k is None: return seq\n",
    "    if len(seq) < k: seq = seq + (\"N\" * (k - len(seq)))\n",
    "    return \" \".join(seq[i:i+k] for i in range(0, len(seq)-k+1))\n",
    "\n",
    "def _build_tokenizer(model_name: str):\n",
    "    return BertTokenizerFast.from_pretrained(model_name, do_lower_case=False)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _encode_pooled(encoder, input_ids, attention_mask):\n",
    "    out = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    hs = out.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).type_as(hs)\n",
    "    summed = (hs * mask).sum(dim=1); denom = mask.sum(dim=1).clamp(min=1e-6)\n",
    "    return summed / denom\n",
    "\n",
    "def _tokenize_batch(tokenizer, seqs, kmer, max_len):\n",
    "    texts = [_kmerize(s, kmer) for s in seqs]\n",
    "    toks = tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "    return toks[\"input_ids\"], toks[\"attention_mask\"]\n",
    "\n",
    "@torch.no_grad()\n",
    "def zero_shot_scores(ds, batch_size=BATCH_SIZE, device=DEVICE):\n",
    "    cfg = BertConfig.from_pretrained(MODEL_NAME)\n",
    "    base_enc = BertModel.from_pretrained(MODEL_NAME, config=cfg).to(device)\n",
    "    base_enc.eval()\n",
    "    tokenizer = _build_tokenizer(MODEL_NAME)\n",
    "\n",
    "    def collate(batch):\n",
    "        ref_ids, ref_msk = _tokenize_batch(tokenizer, [b[\"ref_seq\"] for b in batch], KMER, MAX_LEN)\n",
    "        alt_ids, alt_msk = _tokenize_batch(tokenizer, [b[\"alt_seq\"] for b in batch], KMER, MAX_LEN)\n",
    "        y = torch.tensor([int(b[\"labels\"]) for b in batch], dtype=torch.long)\n",
    "        return {\"ref_input_ids\": ref_ids, \"ref_attention_mask\": ref_msk,\n",
    "                \"alt_input_ids\": alt_ids, \"alt_attention_mask\": alt_msk, \"labels\": y}\n",
    "\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "    all_cos, all_l2, all_y = [], [], []\n",
    "    for b in tqdm(loader, desc=\"Zero-shot scoring\"):\n",
    "        ref_ids = b[\"ref_input_ids\"].to(device); ref_msk = b[\"ref_attention_mask\"].to(device)\n",
    "        alt_ids = b[\"alt_input_ids\"].to(device); alt_msk = b[\"alt_attention_mask\"].to(device)\n",
    "        y = b[\"labels\"].cpu().numpy()\n",
    "\n",
    "        ref_repr = _encode_pooled(base_enc, ref_ids, ref_msk)\n",
    "        alt_repr = _encode_pooled(base_enc, alt_ids, alt_msk)\n",
    "\n",
    "        ref_norm = torch.nn.functional.normalize(ref_repr, dim=-1)\n",
    "        alt_norm = torch.nn.functional.normalize(alt_repr, dim=-1)\n",
    "        cos_sim = (ref_norm * alt_norm).sum(dim=-1)\n",
    "        cos_dist = (1 - cos_sim).cpu().numpy()\n",
    "        l2 = torch.norm(alt_repr - ref_repr, dim=-1).cpu().numpy()\n",
    "\n",
    "        all_cos.append(cos_dist); all_l2.append(l2); all_y.append(y)\n",
    "\n",
    "    return {\"cos_dist\": np.concatenate(all_cos),\n",
    "            \"l2\": np.concatenate(all_l2),\n",
    "            \"y\": np.concatenate(all_y).astype(int)}\n",
    "\n",
    "def _cls_metrics(y_true, scores, thr):\n",
    "    preds = (scores >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, preds, average=\"binary\", zero_division=0)\n",
    "    return {\"accuracy\": float(acc), \"f1\": float(f1), \"precision\": float(p), \"recall\": float(r)}\n",
    "\n",
    "def _best_thr_by_f1(y_true, scores, n=501):\n",
    "    lo, hi = float(scores.min()), float(scores.max())\n",
    "    lo, hi = lo - 1e-6, hi + 1e-6\n",
    "    thrs = np.linspace(lo, hi, n)\n",
    "    best = {\"thr\": 0.5, \"f1\": -1.0, \"precision\": 0.0, \"recall\": 0.0}\n",
    "    for t in thrs:\n",
    "        m = _cls_metrics(y_true, scores, t)\n",
    "        if m[\"f1\"] > best[\"f1\"]:\n",
    "            best = {\"thr\": float(t), **m}\n",
    "    return best\n",
    "\n",
    "def zero_shot_eval_full(val_ds, test_ds, batch_size=BATCH_SIZE):\n",
    "    val = zero_shot_scores(val_ds, batch_size=max(8, batch_size))\n",
    "    test = zero_shot_scores(test_ds, batch_size=max(8, batch_size))\n",
    "    report = {}\n",
    "    for name in [\"cos_dist\", \"l2\"]:\n",
    "        report[(\"val\", name, \"auroc\")] = roc_auc_score(val[\"y\"], val[name]) if len(set(val[\"y\"]))>1 else float(\"nan\")\n",
    "        report[(\"val\", name, \"auprc\")] = average_precision_score(val[\"y\"], val[name]) if len(set(val[\"y\"]))>1 else float(\"nan\")\n",
    "        report[(\"test\", name, \"auroc\")] = roc_auc_score(test[\"y\"], test[name]) if len(set(test[\"y\"]))>1 else float(\"nan\")\n",
    "        report[(\"test\", name, \"auprc\")] = average_precision_score(test[\"y\"], test[name]) if len(set(test[\"y\"]))>1 else float(\"nan\")\n",
    "        best = _best_thr_by_f1(val[\"y\"], val[name])\n",
    "        val_m = _cls_metrics(val[\"y\"], val[name], best[\"thr\"]); tst_m = _cls_metrics(test[\"y\"], test[name], best[\"thr\"])\n",
    "        report[(\"val\", name, \"thr\")] = best[\"thr\"]\n",
    "        for k, v in val_m.items(): report[(\"val\", name, k)] = v\n",
    "        for k, v in tst_m.items(): report[(\"test\", name, k)] = v\n",
    "\n",
    "    def _fmt(split, metric):\n",
    "        cd = report[(split, \"cos_dist\", metric)]; l2 = report[(split, \"l2\", metric)]\n",
    "        return f\"cos={cd:.3f} | l2={l2:.3f}\"\n",
    "    print(\"[Zero-shot: AUROC]   val:\", _fmt(\"val\",\"auroc\"), \"  test:\", _fmt(\"test\",\"auroc\"))\n",
    "    print(\"[Zero-shot: AUPRC]   val:\", _fmt(\"val\",\"auprc\"), \"  test:\", _fmt(\"test\",\"auprc\"))\n",
    "    print(f\"[Zero-shot: Thr(F1) ] val: cos={report[('val','cos_dist','thr')]:.4f} | l2={report[('val','l2','thr')]:.4f}\")\n",
    "    for m in [\"accuracy\",\"f1\",\"precision\",\"recall\"]:\n",
    "        print(f\"[Zero-shot: {m.title():7}] val: {_fmt('val', m)}  test: {_fmt('test', m)}\")\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c65d794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd957c421b1c4f3abeed57132a3fb877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa933cbcef74e7f938d5cbd54c2559e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/359M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f51487a00541dd9555aaffc1934952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c0da1ec549487a85d155d7f17ec870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4f110d5c2348a9837e5e17b82968a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Zero-shot scoring: 100%|██████████| 9571/9571 [29:25<00:00,  5.42it/s]\n",
      "Zero-shot scoring: 100%|██████████| 9571/9571 [29:21<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Zero-shot: AUROC]   val: cos=0.524 | l2=0.520   test: cos=0.524 | l2=0.521\n",
      "[Zero-shot: AUPRC]   val: cos=0.224 | l2=0.223   test: cos=0.224 | l2=0.224\n",
      "[Zero-shot: Thr(F1) ] val: cos=-0.0000 | l2=-0.0000\n",
      "[Zero-shot: Accuracy] val: cos=0.211 | l2=0.211  test: cos=0.211 | l2=0.211\n",
      "[Zero-shot: F1     ] val: cos=0.349 | l2=0.349  test: cos=0.349 | l2=0.349\n",
      "[Zero-shot: Precision] val: cos=0.211 | l2=0.211  test: cos=0.211 | l2=0.211\n",
      "[Zero-shot: Recall ] val: cos=1.000 | l2=1.000  test: cos=1.000 | l2=1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('val', 'cos_dist', 'auroc'): 0.523705824967194,\n",
       " ('val', 'cos_dist', 'auprc'): 0.2239033612626116,\n",
       " ('test', 'cos_dist', 'auroc'): 0.523583753264157,\n",
       " ('test', 'cos_dist', 'auprc'): 0.2243609812005871,\n",
       " ('val', 'cos_dist', 'thr'): -1.2384185791015625e-06,\n",
       " ('val', 'cos_dist', 'accuracy'): 0.21111151021581043,\n",
       " ('val', 'cos_dist', 'f1'): 0.3486243973987037,\n",
       " ('val', 'cos_dist', 'precision'): 0.21111151021581043,\n",
       " ('val', 'cos_dist', 'recall'): 1.0,\n",
       " ('test', 'cos_dist', 'accuracy'): 0.21111082085409577,\n",
       " ('test', 'cos_dist', 'f1'): 0.34862345743920753,\n",
       " ('test', 'cos_dist', 'precision'): 0.21111082085409577,\n",
       " ('test', 'cos_dist', 'recall'): 1.0,\n",
       " ('val', 'l2', 'auroc'): 0.5203423088666612,\n",
       " ('val', 'l2', 'auprc'): 0.222761176965432,\n",
       " ('test', 'l2', 'auroc'): 0.5212464971094904,\n",
       " ('test', 'l2', 'auprc'): 0.22370426651033742,\n",
       " ('val', 'l2', 'thr'): -1e-06,\n",
       " ('val', 'l2', 'accuracy'): 0.21111151021581043,\n",
       " ('val', 'l2', 'f1'): 0.3486243973987037,\n",
       " ('val', 'l2', 'precision'): 0.21111151021581043,\n",
       " ('val', 'l2', 'recall'): 1.0,\n",
       " ('test', 'l2', 'accuracy'): 0.21111082085409577,\n",
       " ('test', 'l2', 'f1'): 0.34862345743920753,\n",
       " ('test', 'l2', 'precision'): 0.21111082085409577,\n",
       " ('test', 'l2', 'recall'): 1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Execute zero-shot\n",
    "val_ds  = SNVDataset(df_val)\n",
    "test_ds = SNVDataset(df_test)\n",
    "zs_report = zero_shot_eval_full(val_ds, test_ds, batch_size=BATCH_SIZE)\n",
    "zs_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c60b1e-a346-425f-89dd-520bb97abe9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
