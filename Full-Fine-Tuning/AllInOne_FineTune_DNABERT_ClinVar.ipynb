{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b65762",
   "metadata": {},
   "source": [
    "\n",
    "# ClinVar Variant Effect Prediction with DNABERT — Three Fine-Tuning Setups\n",
    "\n",
    "This notebook runs **three** supervised approaches on the same train/val/test split and then prints a side‑by‑side comparison:\n",
    "\n",
    "1. **Joint Paired Input** (single pass): `BertForSequenceClassification` with `(ref, alt)` passed as a **paired** sequence (segment IDs 0/1).  \n",
    "2. **Dual-Input (Concat)**: Shared encoder encodes ref and alt **separately**, concatenates pooled embeddings → MLP classifier.  \n",
    "3. **Siamese (Delta)**: Shared encoder encodes ref and alt separately and fuses **[ref, alt, (alt−ref), |alt−ref|, ref⊙alt]** → MLP classifier.\n",
    "\n",
    "> Assumes you already have a prepared CSV with columns: `ref_seq`, `alt_seq`, `label` (e.g., from the hg38 prep notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a67365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Configuration ===\n",
    "import os\n",
    "\n",
    "VAL_CSV = os.path.expanduser(\"~/data/clinvar_seq_pairs_hg38_flank100.csv\")  # change if needed\n",
    "MODEL_NAME = \"zhihan1996/DNA_bert_6\"   # DNABERT v1 (6-mer)\n",
    "KMER       = 6\n",
    "MAX_LEN    = 512\n",
    "SEED       = 42\n",
    "\n",
    "# Training\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE  = 32\n",
    "LR               = 3e-5\n",
    "WEIGHT_DECAY     = 0.01\n",
    "NUM_EPOCHS       = 3\n",
    "WARMUP_RATIO     = 0.06\n",
    "FP16             = True\n",
    "\n",
    "OUT_ROOT = \"./dnabert_all_ft\"\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"Config OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6463a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, install deps:\n",
    "# %pip install --quiet transformers==4.44.2 torch==2.4.0 pandas==2.1.4 scikit-learn==1.3.2 numpy==1.26.4 tqdm==4.66.4 evaluate==0.4.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(VAL_CSV, dtype={\"CHROM\": str})\n",
    "assert {\"ref_seq\",\"alt_seq\"}.issubset(df.columns), f\"Missing sequence cols in {VAL_CSV}\"\n",
    "\n",
    "# Ensure binary label\n",
    "if \"label\" not in df.columns:\n",
    "    if \"LABEL\" in df.columns:\n",
    "        df[\"label\"] = df[\"LABEL\"].astype(int)\n",
    "    elif \"CLNSIG\" in df.columns:\n",
    "        def clinsig_to_label(v):\n",
    "            s = str(v).lower()\n",
    "            has_path=(\"pathogenic\" in s); has_ben=(\"benign\" in s)\n",
    "            if has_path and not has_ben: return 1\n",
    "            if has_ben and not has_path: return 0\n",
    "            return np.nan\n",
    "        df[\"label\"] = df[\"CLNSIG\"].apply(clinsig_to_label).astype(\"float\")\n",
    "    else:\n",
    "        raise KeyError(\"Need label/LABEL/CLNSIG in CSV to derive labels.\")\n",
    "\n",
    "before=len(df)\n",
    "df = df.dropna(subset=[\"label\"]).copy()\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "after=len(df)\n",
    "print(f\"Using {after}/{before} labeled rows.\")\n",
    "print(\"Class balance:\", df['label'].value_counts(normalize=True).to_dict())\n",
    "\n",
    "# Freeze a single split reused by all models\n",
    "df_train, df_tmp = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df[\"label\"])\n",
    "df_val, df_test  = train_test_split(df_tmp, test_size=0.5, random_state=SEED, stratify=df_tmp[\"label\"])\n",
    "print(\"Splits:\", df_train.shape, df_val.shape, df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5342fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, math\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, BertModel, BertConfig\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "\n",
    "def kmerize(seq, k=KMER):\n",
    "    s = str(seq).strip().upper().replace(\"U\",\"T\")\n",
    "    if len(s) < k: s += \"N\"*(k-len(s))\n",
    "    return \" \".join(s[i:i+k] for i in range(0, len(s)-k+1))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.tensor(logits).softmax(dim=-1).numpy()\n",
    "    preds = probs.argmax(axis=1)\n",
    "    pos = probs[:,1]\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", zero_division=0)\n",
    "    try: auroc = roc_auc_score(labels, pos)\n",
    "    except: auroc = float(\"nan\")\n",
    "    try: auprc = average_precision_score(labels, pos)\n",
    "    except: auprc = float(\"nan\")\n",
    "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1, \"auroc\": auroc, \"auprc\": auprc}\n",
    "\n",
    "# Imbalance weights from TRAIN only\n",
    "classes = np.array([0,1])\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=df_train[\"label\"].values)\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "print(\"Class weights:\", class_weights.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset A: Joint paired input (single pass with token_type_ids distinguishing ref/alt)\n",
    "class JointPairDataset(Dataset):\n",
    "    def __init__(self, frame):\n",
    "        self.df = frame.reset_index(drop=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        a = kmerize(r[\"ref_seq\"]); b = kmerize(r[\"alt_seq\"])\n",
    "        enc = tokenizer(a, b, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(int(r[\"label\"]), dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Dataset B: Separate encodings (for Dual-Input & Siamese)\n",
    "class SepPairDataset(Dataset):\n",
    "    def __init__(self, frame):\n",
    "        self.df = frame.reset_index(drop=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        ref_txt = kmerize(r[\"ref_seq\"]); alt_txt = kmerize(r[\"alt_seq\"])\n",
    "        ref_enc = tokenizer(ref_txt, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "        alt_enc = tokenizer(alt_txt, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "        item = {\n",
    "            \"ref_input_ids\": ref_enc[\"input_ids\"].squeeze(0),\n",
    "            \"ref_attention_mask\": ref_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"alt_input_ids\": alt_enc[\"input_ids\"].squeeze(0),\n",
    "            \"alt_attention_mask\": alt_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(int(r[\"label\"]), dtype=torch.long),\n",
    "        }\n",
    "        if \"token_type_ids\" in ref_enc:\n",
    "            item[\"ref_token_type_ids\"] = ref_enc[\"token_type_ids\"].squeeze(0)\n",
    "            item[\"alt_token_type_ids\"] = alt_enc[\"token_type_ids\"].squeeze(0)\n",
    "        return item\n",
    "\n",
    "train_joint, val_joint, test_joint = JointPairDataset(df_train), JointPairDataset(df_val), JointPairDataset(df_test)\n",
    "train_sep,   val_sep,  test_sep    = SepPairDataset(df_train),  SepPairDataset(df_val),  SepPairDataset(df_test)\n",
    "\n",
    "len(train_joint), len(val_joint), len(test_joint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d5a7b",
   "metadata": {},
   "source": [
    "## Step 1 — Fine-tune: Joint Paired Input (single-pass BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "joint_model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "args_joint = TrainingArguments(\n",
    "    output_dir=f\"{OUT_ROOT}/joint\",\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auroc\",\n",
    "    greater_is_better=True,\n",
    "    fp16=FP16,\n",
    "    logging_steps=100,\n",
    "    seed=SEED,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer_joint = WeightedTrainer(\n",
    "    model=joint_model,\n",
    "    args=args_joint,\n",
    "    train_dataset=train_joint,\n",
    "    eval_dataset=val_joint,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Training joint model...\")\n",
    "joint_train = trainer_joint.train()\n",
    "joint_val   = trainer_joint.evaluate(eval_dataset=val_joint)\n",
    "joint_test  = trainer_joint.evaluate(eval_dataset=test_joint)\n",
    "print(\"Joint Val:\", joint_val)\n",
    "print(\"Joint Test:\", joint_test)\n",
    "\n",
    "# store for summary\n",
    "results = []\n",
    "results.append({\"model\":\"joint\", **{f\"val_{k}\":v for k,v in joint_val.items()}, **{f\"test_{k}\":v for k,v in joint_test.items()}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85821247",
   "metadata": {},
   "source": [
    "## Step 2 — Fine-tune: Dual-Input (Concat) with Shared Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dafb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_mean(hs, mask):\n",
    "    mask = mask.unsqueeze(-1).type_as(hs)\n",
    "    summed = (hs * mask).sum(dim=1)\n",
    "    denom = mask.sum(dim=1).clamp(min=1e-6)\n",
    "    return summed / denom\n",
    "\n",
    "class SiameseFusion(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME, num_labels=2, fusion=\"concat\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.cfg = BertConfig.from_pretrained(model_name)\n",
    "        self.enc = BertModel.from_pretrained(model_name, config=self.cfg)\n",
    "        self.fusion = fusion\n",
    "        hidden = self.cfg.hidden_size\n",
    "        if fusion == \"concat\":\n",
    "            in_dim = hidden * 2\n",
    "        elif fusion == \"delta\":\n",
    "            in_dim = hidden * 5   # [ref, alt, Δ, |Δ|, ref⊙alt]\n",
    "        else:\n",
    "            raise ValueError(\"fusion must be 'concat' or 'delta'\")\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, **batch):\n",
    "        ref_ids = batch[\"ref_input_ids\"]; ref_msk = batch[\"ref_attention_mask\"]\n",
    "        alt_ids = batch[\"alt_input_ids\"]; alt_msk = batch[\"alt_attention_mask\"]\n",
    "        ref_tti = batch.get(\"ref_token_type_ids\", None)\n",
    "        alt_tti = batch.get(\"alt_token_type_ids\", None)\n",
    "\n",
    "        ref_out = self.enc(input_ids=ref_ids, attention_mask=ref_msk, token_type_ids=ref_tti)\n",
    "        alt_out = self.enc(input_ids=alt_ids, attention_mask=alt_msk, token_type_ids=alt_tti)\n",
    "\n",
    "        ref_pool = masked_mean(ref_out.last_hidden_state, ref_msk)\n",
    "        alt_pool = masked_mean(alt_out.last_hidden_state, alt_msk)\n",
    "\n",
    "        if self.fusion == \"concat\":\n",
    "            feat = torch.cat([ref_pool, alt_pool], dim=-1)\n",
    "        else:\n",
    "            delta = alt_pool - ref_pool\n",
    "            feat = torch.cat([ref_pool, alt_pool, delta, delta.abs(), ref_pool * alt_pool], dim=-1)\n",
    "\n",
    "        logits = self.head(feat)\n",
    "        return {\"logits\": logits, \"labels\": batch[\"labels\"]}\n",
    "\n",
    "# Concat model\n",
    "concat_model = SiameseFusion(fusion=\"concat\")\n",
    "\n",
    "args_concat = TrainingArguments(\n",
    "    output_dir=f\"{OUT_ROOT}/concat\",\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auroc\",\n",
    "    greater_is_better=True,\n",
    "    fp16=FP16,\n",
    "    logging_steps=100,\n",
    "    seed=SEED,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "class WeightedTrainer2(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs[\"labels\"]\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[\"logits\"]\n",
    "        loss = nn.CrossEntropyLoss(weight=class_weights.to(logits.device))(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer_concat = WeightedTrainer2(\n",
    "    model=concat_model,\n",
    "    args=args_concat,\n",
    "    train_dataset=train_sep,\n",
    "    eval_dataset=val_sep,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Training Dual-Input (Concat)...\")\n",
    "concat_train = trainer_concat.train()\n",
    "concat_val   = trainer_concat.evaluate(eval_dataset=val_sep)\n",
    "concat_test  = trainer_concat.evaluate(eval_dataset=test_sep)\n",
    "print(\"Concat Val:\", concat_val)\n",
    "print(\"Concat Test:\", concat_test)\n",
    "\n",
    "results.append({\"model\":\"concat\", **{f\"val_{k}\":v for k,v in concat_val.items()}, **{f\"test_{k}\":v for k,v in concat_test.items()}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d4e08",
   "metadata": {},
   "source": [
    "## Step 3 — Fine-tune: Siamese (Delta) with Shared Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fedb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reuse SiameseFusion with fusion='delta'\n",
    "siamese_model = SiameseFusion(fusion=\"delta\")\n",
    "\n",
    "args_siamese = TrainingArguments(\n",
    "    output_dir=f\"{OUT_ROOT}/siamese\",\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auroc\",\n",
    "    greater_is_better=True,\n",
    "    fp16=FP16,\n",
    "    logging_steps=100,\n",
    "    seed=SEED,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "trainer_siamese = WeightedTrainer2(\n",
    "    model=siamese_model,\n",
    "    args=args_siamese,\n",
    "    train_dataset=train_sep,\n",
    "    eval_dataset=val_sep,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Training Siamese (Delta)...\")\n",
    "siam_train = trainer_siamese.train()\n",
    "siam_val   = trainer_siamese.evaluate(eval_dataset=val_sep)\n",
    "siam_test  = trainer_siamese.evaluate(eval_dataset=test_sep)\n",
    "print(\"Siamese Val:\", siam_val)\n",
    "print(\"Siamese Test:\", siam_test)\n",
    "\n",
    "results.append({\"model\":\"siamese_delta\", **{f\"val_{k}\":v for k,v in siam_val.items()}, **{f\"test_{k}\":v for k,v in siam_test.items()}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86ccec",
   "metadata": {},
   "source": [
    "## Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "cmp = pd.DataFrame(results)\n",
    "\n",
    "# Keep the most important columns\n",
    "cols = [\"model\",\n",
    "        \"val_auroc\",\"test_auroc\",\n",
    "        \"val_auprc\",\"test_auprc\",\n",
    "        \"val_accuracy\",\"test_accuracy\",\n",
    "        \"val_f1\",\"test_f1\",\n",
    "        \"val_precision\",\"test_precision\",\n",
    "        \"val_recall\",\"test_recall\"]\n",
    "existing = [c for c in cols if c in cmp.columns]\n",
    "cmp = cmp[existing].copy()\n",
    "\n",
    "display(cmp)\n",
    "cmp.to_csv(f\"{OUT_ROOT}/comparison_metrics.csv\", index=False)\n",
    "print(\"Saved:\", f\"{OUT_ROOT}/comparison_metrics.csv\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
